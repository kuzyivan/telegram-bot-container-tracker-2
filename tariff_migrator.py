# tariff_migrator.py
print("üöÄ –ó–ê–ü–£–°–ö –°–ö–†–ò–ü–¢–ê...")

import asyncio
import os
import re
import pandas as pd
import numpy as np
import sys
import glob
from dotenv import load_dotenv
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column
from sqlalchemy import String, Integer, ARRAY, Index, UniqueConstraint
from sqlalchemy.dialects.postgresql import insert as pg_insert
import logging
from io import StringIO 

# --- 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ .env ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
log = logging.getLogger(__name__)

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ zdtarif_bot/data
current_file_path = os.path.abspath(__file__)
project_root_dir = os.path.dirname(current_file_path)
sys.path.insert(0, project_root_dir)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()
TARIFF_DB_URL = os.getenv("TARIFF_DATABASE_URL")

# --- 2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ ORM –ú–æ–¥–µ–ª–µ–π –¥–ª—è –Ω–æ–≤–æ–π –ë–î ---

class Base(DeclarativeBase):
    pass

class TariffStation(Base):
    '''
    –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ 2-–†–ü.csv.
    '''
    __tablename__ = 'tariff_stations'
    id: Mapped[int] = mapped_column(primary_key=True)
    name: Mapped[str] = mapped_column(String, index=True) 
    code: Mapped[str] = mapped_column(String(6), index=True, unique=True) 
    railway: Mapped[str | None] = mapped_column(String)
    operations: Mapped[str | None] = mapped_column(String)
    transit_points: Mapped[list[str] | None] = mapped_column(ARRAY(String)) 

    __table_args__ = (
        Index('ix_tariff_stations_name_code', 'name', 'code'),
    )

class TariffMatrix(Base):
    '''
    –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ 3-*.csv.
    '''
    __tablename__ = 'tariff_matrix'
    id: Mapped[int] = mapped_column(primary_key=True)
    station_a: Mapped[str] = mapped_column(String, index=True)
    station_b: Mapped[str] = mapped_column(String, index=True)
    distance: Mapped[int] = mapped_column(Integer)

    __table_args__ = (
        UniqueConstraint('station_a', 'station_b', name='uq_station_pair'),
    )

# --- 3. –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ ---

def parse_transit_points_for_db(tp_string: str) -> list[str]:
    '''
    –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–æ–∫—É —Ç—Ä–∞–Ω–∑–∏—Ç–Ω—ã—Ö –ø—É–Ω–∫—Ç–æ–≤ –∏–∑ 2-–†–ü.csv –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫.
    '''
    if not isinstance(tp_string, str) or not tp_string:
        return []
    
    pattern = re.compile(r'(\d{6})\s(.*?)\s-\s(\d+)–∫–º')
    matches = pattern.findall(tp_string)
    
    transit_points_str = []
    for match in matches:
        transit_points_str.append(f"{match[0]}:{match[1].strip()}:{int(match[2])}")
        
    return transit_points_str

def load_kniga_2_rp(filepath: str) -> pd.DataFrame | None:
    '''
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç 2-–†–ü.csv –∏–∑ zdtarif_bot/data
    '''
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –Ω–∞—á–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–ø—É—Å–∫–∞—è —à–∞–ø–∫—É
        with open(filepath, 'r', encoding='cp1251') as f:
            lines = f.readlines()
        
        start_row = 0
        for i, line in enumerate(lines[:20]):
            if "–ö–æ–¥ —Å—Ç–∞–Ω—Ü–∏–∏" in line or "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ" in line:
                start_row = i + 1
                break
        
        if start_row == 0: start_row = 6

        df = pd.read_csv(
            filepath,
            skiprows=start_row,
            names=[
                'num', 'station_name', 'operations', 'railway', 
                'transit_points_raw', 'station_code'
            ],
            encoding='cp1251',
            dtype={'station_code': str},
            on_bad_lines='skip'
        )
        df['station_name'] = df['station_name'].str.strip()
        df['station_code'] = df['station_code'].str.strip()
        df['railway'] = df['railway'].str.strip()
        df['operations'] = df['operations'].str.strip()

        df.dropna(subset=['station_name', 'station_code'], inplace=True)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –ö–û–î–£
        df.drop_duplicates(subset=['station_code'], keep='first', inplace=True)
        
        log.info(f"‚úÖ –§–∞–π–ª —Å—Ç–∞–Ω—Ü–∏–π {os.path.basename(filepath)} –∑–∞–≥—Ä—É–∂–µ–Ω, {len(df)} –∑–∞–ø–∏—Å–µ–π.")
        return df
    except FileNotFoundError:
        log.error(f"‚ùå –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª '{filepath}'.")
        return None
    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {filepath}: {e}", exc_info=True)
        return None

def load_kniga_3_matrix(filepath: str) -> pd.DataFrame | None:
    '''
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É (3-*.csv) –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –µ–µ –≤ "–¥–ª–∏–Ω–Ω—ã–π" —Ñ–æ—Ä–º–∞—Ç.
    '''
    try:
        with open(filepath, 'r', encoding='cp1251') as f:
            lines = f.readlines()

        # 1. –ü–æ–∏—Å–∫ –≥—Ä–∞–Ω–∏—Ü –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ –¥–∞–Ω–Ω—ã—Ö
        header_start_line = -1
        data_start_line = -1
        
        for i, line in enumerate(lines):
            if "–ö–æ–Ω–µ—á–Ω—ã–π –ø—É–Ω–∫—Ç –º–∞—Ä—à—Ä—É—Ç–∞" in line and header_start_line == -1:
                header_start_line = i + 1 
            if "‚Ññ –ø/–ø" in line and "–ù–∞—á–∞–ª—å–Ω—ã–π –ø—É–Ω–∫—Ç" in line:
                data_start_line = i
                break
        
        if header_start_line == -1 or data_start_line == -1:
            log.error(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω—ã –º–∞—Ä–∫–µ—Ä—ã –Ω–∞—á–∞–ª–∞ –≤ {filepath}.")
            return None

        # 2. –ü–∞—Ä—Å–∏–Ω–≥ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        header_lines = lines[header_start_line:data_start_line]
        header_cols = {}
        
        for line in header_lines:
            cleaned_line = line.rstrip(',\n')
            cols = cleaned_line.split(',')
            for col_idx in range(2, len(cols)): 
                val = cols[col_idx].strip()
                if val:
                    if col_idx not in header_cols: header_cols[col_idx] = []
                    header_cols[col_idx].append(val)
        
        data_header_row = lines[data_start_line].strip().split(',')
        column_name_to_station_map = {}
        
        for col_idx, station_name_parts in header_cols.items():
            if col_idx < len(data_header_row):
                col_name_in_df = data_header_row[col_idx].strip()
                full_name = " ".join(station_name_parts)
                full_name = re.sub(r'\s+', ' ', full_name).strip()
                column_name_to_station_map[col_name_in_df] = full_name

        if not column_name_to_station_map:
             log.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –∫–∞—Ä—Ç—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è {filepath}.")
             return None
        
        log.info(f"–§–∞–π–ª {os.path.basename(filepath)}: –Ω–∞–π–¥–µ–Ω–æ {len(column_name_to_station_map)} —Ü–µ–ª–µ–≤—ã—Ö —Å—Ç–∞–Ω—Ü–∏–π.")

        # 3. –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        data_io = StringIO("".join(lines[data_start_line:]))
        df = pd.read_csv(data_io, header=0, encoding='cp1251', on_bad_lines='skip')
        
        df.rename(columns={df.columns[0]: 'num_pp', df.columns[1]: 'station_a'}, inplace=True)

        # 4. –°–∫–ª–µ–∏–≤–∞–Ω–∏–µ —Ä–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–∞–Ω—Ü–∏–π –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        df = df.replace({np.nan: None})
        rows_to_drop = []
        
        for i in range(len(df)):
            if i == 0: continue
            curr_num = df.iloc[i]['num_pp']
            curr_name = str(df.iloc[i]['station_a'] or '').strip()
            
            if not curr_num and curr_name:
                prev_idx = i - 1
                while prev_idx in rows_to_drop and prev_idx >= 0:
                    prev_idx -= 1
                
                if prev_idx >= 0:
                    prev_name = str(df.iloc[prev_idx]['station_a']).strip()
                    df.iloc[prev_idx, 1] = f"{prev_name} {curr_name}".strip()
                    rows_to_drop.append(i)
            elif not curr_num and not curr_name:
                rows_to_drop.append(i)

        df.drop(df.index[rows_to_drop], inplace=True)
        
        # 5. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –¥–ª–∏–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        valid_value_vars = [c for c in df.columns if c in column_name_to_station_map]
        
        df_long = df.melt(
            id_vars=['station_a'], 
            value_vars=valid_value_vars, 
            var_name='station_b_key', 
            value_name='distance'
        )
        
        # 6. –û—á–∏—Å—Ç–∫–∞
        df_long = df_long[pd.to_numeric(df_long['distance'], errors='coerce').notna()]
        df_long['distance'] = df_long['distance'].astype(int)
        df_long = df_long[df_long['distance'] > 0]
        
        df_long['station_a'] = df_long['station_a'].astype(str).str.strip()
        df_long['station_b'] = df_long['station_b_key'].map(column_name_to_station_map)
        
        df_long.dropna(subset=['station_b', 'station_a'], inplace=True)
        
        final_df = df_long[['station_a', 'station_b', 'distance']].copy()
        
        log.info(f"‚úÖ {os.path.basename(filepath)} –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {len(final_df)} –º–∞—Ä—à—Ä—É—Ç–æ–≤.")
        return final_df
        
    except Exception as e:
        log.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ {filepath}: {e}", exc_info=True)
        return None

# --- 4. –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –º–∏–≥—Ä–∞—Ü–∏–∏ ---

async def main_migrate():
    '''
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è. –ü–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è, –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã, –∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ.
    '''
    if not TARIFF_DB_URL:
        log.error("‚ùå TARIFF_DATABASE_URL –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ .env —Ñ–∞–π–ª–µ. –ú–∏–≥—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞.")
        return
        
    log.info(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –Ω–æ–≤–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —Ç–∞—Ä–∏—Ñ–æ–≤...")
    
    # 1. –°–æ–∑–¥–∞–µ–º –¥–≤–∏–∂–æ–∫ –∏ —Ç–∞–±–ª–∏—Ü—ã
    engine = create_async_engine(TARIFF_DB_URL)
    async with engine.begin() as conn:
        log.info("–û—á–∏—Å—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–∞–±–ª–∏—Ü (Drop All)...")
        await conn.run_sync(Base.metadata.drop_all)
        log.info("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ç–∞–±–ª–∏—Ü (Create All)...")
        await conn.run_sync(Base.metadata.create_all)
    
    Session = async_sessionmaker(engine, expire_on_commit=False)
    
    # –ò—â–µ–º –ø–∞–ø–∫—É —Å –¥–∞–Ω–Ω—ã–º–∏
    data_dir_path = os.path.join(project_root_dir, 'zdtarif_bot', 'data')
    if not os.path.exists(data_dir_path):
        data_dir_path = os.path.join(project_root_dir, 'data')
        if not os.path.exists(data_dir_path):
             log.error(f"‚ùå –ù–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –ø–∞–ø–∫—É 'data' –∏–ª–∏ 'zdtarif_bot/data' –≤ {project_root_dir}")
             await engine.dispose()
             return
    
    log.info(f"–ò—Å–ø–æ–ª—å–∑—É—é –ø–∞–ø–∫—É —Å –¥–∞–Ω–Ω—ã–º–∏: {data_dir_path}")

    # --- 1. –ú–∏–≥—Ä–∞—Ü–∏—è –°—Ç–∞–Ω—Ü–∏–π ---
    log.info("--- 1/2: –ù–∞—á–∏–Ω–∞—é –º–∏–≥—Ä–∞—Ü–∏—é –°—Ç–∞–Ω—Ü–∏–π ---")
    
    station_files = glob.glob(os.path.join(data_dir_path, '2-–†–ü*.csv'))
    all_stations_dfs = []
    for filepath in station_files:
        df = load_kniga_2_rp(filepath)
        if df is not None: all_stations_dfs.append(df)

    if all_stations_dfs:
        stations_df = pd.concat(all_stations_dfs, ignore_index=True)
        stations_df.drop_duplicates(subset=['station_code'], keep='first', inplace=True)
        
        total_stations = len(stations_df)
        log.info(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {total_stations} –£–ù–ò–ö–ê–õ–¨–ù–´–• —Å—Ç–∞–Ω—Ü–∏–π.")
        
        stations_df = stations_df.where(pd.notnull(stations_df), None)

        async with Session() as session:
            # ‚úÖ BATCH SIZE –£–ú–ï–ù–¨–®–ï–ù –î–û 1000
            batch_size = 1000 
            for start in range(0, total_stations, batch_size):
                end = min(start + batch_size, total_stations)
                batch = stations_df.iloc[start:end]
                
                values = []
                for _, row in batch.iterrows():
                    values.append({
                        'name': row['station_name'],
                        'code': row['station_code'],
                        'railway': row['railway'],
                        'operations': row['operations'],
                        'transit_points': parse_transit_points_for_db(row['transit_points_raw'])
                    })
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º insert().on_conflict_do_nothing()
                stmt = pg_insert(TariffStation).values(values).on_conflict_do_nothing(
                    index_elements=['code']
                )
                await session.execute(stmt)
                await session.commit()
                log.info(f"–°—Ç–∞–Ω—Ü–∏–∏: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {end}/{total_stations}")
    else:
        log.warning("‚ùå –§–∞–π–ª—ã —Å—Ç–∞–Ω—Ü–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")

    # --- 2. –ú–∏–≥—Ä–∞—Ü–∏—è –ú–∞—Ç—Ä–∏—Ü ---
    log.info("--- 2/2: –ù–∞—á–∏–Ω–∞—é –º–∏–≥—Ä–∞—Ü–∏—é –ú–∞—Ç—Ä–∏—Ü ---")
    
    all_matrix_files = glob.glob(os.path.join(data_dir_path, '3-*.csv'))
    files_to_exclude = ['3-–í–≤–æ–¥–Ω—ã–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è.csv', '3-–û–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è.csv']
    matrix_files_to_process = [f for f in all_matrix_files if os.path.basename(f) not in files_to_exclude]
    
    all_routes_dfs = []
    for filepath in matrix_files_to_process: 
        matrix_df = load_kniga_3_matrix(filepath)
        if matrix_df is not None and not matrix_df.empty:
            all_routes_dfs.append(matrix_df)

    if not all_routes_dfs:
        log.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –º–∞—Ä—à—Ä—É—Ç–æ–≤ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏.")
    else:
        combined_routes_df = pd.concat(all_routes_dfs, ignore_index=True)
        # –°–∏–º–º–µ—Ç—Ä–∏—è
        log.info("–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω—ã—Ö –º–∞—Ä—à—Ä—É—Ç–æ–≤ (—Å–∏–º–º–µ—Ç—Ä–∏—è)...")
        reversed_routes_df = combined_routes_df.rename(columns={'station_a': 'station_b', 'station_b': 'station_a'})
        final_routes_df = pd.concat([combined_routes_df, reversed_routes_df], ignore_index=True)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        final_routes_df.drop_duplicates(subset=['station_a', 'station_b'], keep='first', inplace=True)

        total_routes_to_add = len(final_routes_df)
        log.info(f"–í—Å–µ–≥–æ –º–∞—Ä—à—Ä—É—Ç–æ–≤ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏: {total_routes_to_add}")
        
        async with Session() as session:
            # ‚úÖ BATCH SIZE –£–ú–ï–ù–¨–®–ï–ù –î–û 1000
            BATCH_SIZE = 1000
            num_batches = (total_routes_to_add + BATCH_SIZE - 1) // BATCH_SIZE
            
            for i in range(num_batches):
                start_index = i * BATCH_SIZE
                end_index = min((i + 1) * BATCH_SIZE, total_routes_to_add)
                
                batch_df = final_routes_df.iloc[start_index:end_index]
                routes_to_insert = batch_df.to_dict(orient='records')
                
                try:
                    async with session.begin():
                        stmt = pg_insert(TariffMatrix).values(routes_to_insert).on_conflict_do_nothing(
                            index_elements=['station_a', 'station_b']
                        )
                        await session.execute(stmt)
                    
                    if (i + 1) % 50 == 0:
                        log.info(f"–ú–∞—Ç—Ä–∏—Ü–∞: –∑–∞–≥—Ä—É–∂–µ–Ω–æ {end_index}/{total_routes_to_add}")
                        
                except Exception as e:
                    log.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–∞–∫–µ—Ç–µ {i}: {e}", exc_info=True)

    log.info("üéâüéâüéâ == –ú–ò–ì–†–ê–¶–ò–Ø –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–ê! ==")
    await engine.dispose()

if __name__ == "__main__":
    asyncio.run(main_migrate())