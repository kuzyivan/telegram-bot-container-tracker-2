# tariff_migrator.py
import asyncio
import os
import re
import pandas as pd
import numpy as np # –î–æ–±–∞–≤–ª–µ–Ω –∏–º–ø–æ—Ä—Ç numpy –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å NaN/None
import sys
import glob
from dotenv import load_dotenv
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column
from sqlalchemy import String, Integer, ARRAY, Index, UniqueConstraint
from sqlalchemy.exc import IntegrityError
from sqlalchemy.dialects.postgresql import insert as pg_insert
import logging
from io import StringIO 

# --- 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ .env ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
log = logging.getLogger(__name__)

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ zdtarif_bot/data
current_file_path = os.path.abspath(__file__)
project_root_dir = os.path.dirname(current_file_path)
sys.path.insert(0, project_root_dir)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è (–æ—Å–æ–±–µ–Ω–Ω–æ TARIFF_DATABASE_URL)
load_dotenv()
TARIFF_DB_URL = os.getenv("TARIFF_DATABASE_URL")

# --- 2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ ORM –ú–æ–¥–µ–ª–µ–π –¥–ª—è –Ω–æ–≤–æ–π –ë–î ---

class Base(DeclarativeBase):
    pass

class TariffStation(Base):
    '''
    –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ 2-–†–ü.csv.
    '''
    __tablename__ = 'tariff_stations'
    id: Mapped[int] = mapped_column(primary_key=True)
    name: Mapped[str] = mapped_column(String, index=True) 
    code: Mapped[str] = mapped_column(String(6), index=True, unique=True) 
    railway: Mapped[str | None] = mapped_column(String)
    operations: Mapped[str | None] = mapped_column(String)
    transit_points: Mapped[list[str] | None] = mapped_column(ARRAY(String)) 

    __table_args__ = (
        Index('ix_tariff_stations_name_code', 'name', 'code'),
    )

class TariffMatrix(Base):
    '''
    –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ 3-*.csv.
    '''
    __tablename__ = 'tariff_matrix'
    id: Mapped[int] = mapped_column(primary_key=True)
    station_a: Mapped[str] = mapped_column(String, index=True)
    station_b: Mapped[str] = mapped_column(String, index=True)
    distance: Mapped[int] = mapped_column(Integer)

    __table_args__ = (
        UniqueConstraint('station_a', 'station_b', name='uq_station_pair'),
    )

# --- 3. –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ ---

def parse_transit_points_for_db(tp_string: str) -> list[str]:
    '''
    –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–æ–∫—É —Ç—Ä–∞–Ω–∑–∏—Ç–Ω—ã—Ö –ø—É–Ω–∫—Ç–æ–≤ –∏–∑ 2-–†–ü.csv –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫.
    '''
    if not isinstance(tp_string, str) or not tp_string:
        return []
    
    pattern = re.compile(r'(\d{6})\s(.*?)\s-\s(\d+)–∫–º')
    matches = pattern.findall(tp_string)
    
    transit_points_str = []
    for match in matches:
        transit_points_str.append(f"{match[0]}:{match[1].strip()}:{int(match[2])}")
        
    return transit_points_str

def load_kniga_2_rp(filepath: str) -> pd.DataFrame | None:
    '''
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç 2-–†–ü.csv –∏–∑ zdtarif_bot/data
    '''
    try:
        df = pd.read_csv(
            filepath,
            skiprows=6, # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
            names=[
                'num', 'station_name', 'operations', 'railway', 
                'transit_points_raw', 'station_code'
            ],
            encoding='cp1251',
            dtype={'station_code': str} 
        )
        df['station_name'] = df['station_name'].str.strip()
        df['station_code'] = df['station_code'].str.strip()
        df['railway'] = df['railway'].str.strip()
        df['operations'] = df['operations'].str.strip()

        df.dropna(subset=['station_name', 'station_code'], inplace=True)
        
        # --- üêû –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –ö–û–î–£, –∞ –Ω–µ –ø–æ –ò–ú–ï–ù–ò üêû ---
        df.drop_duplicates(subset=['station_code'], keep='first', inplace=True)
        # --- üèÅ –ö–û–ù–ï–¶ –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø üèÅ ---
        
        log.info(f"‚úÖ –§–∞–π–ª {os.path.basename(filepath)} –∑–∞–≥—Ä—É–∂–µ–Ω, {len(df)} –£–ù–ò–ö–ê–õ–¨–ù–´–• —Å—Ç–∞–Ω—Ü–∏–π (–ø–æ –∫–æ–¥—É).")
        return df
    except FileNotFoundError:
        log.error(f"‚ùå –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª '{filepath}'.")
        return None
    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {filepath}: {e}", exc_info=True)
        return None

def load_kniga_3_matrix(filepath: str) -> pd.DataFrame | None:
    '''
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É (3-*.csv) –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –µ–µ –≤ "–¥–ª–∏–Ω–Ω—ã–π" —Ñ–æ—Ä–º–∞—Ç,
    –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å—á–∏—Ç—ã–≤–∞—è –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –æ–±—ä–µ–¥–∏–Ω—è—è –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–∞–Ω—Ü–∏–π.
    '''
    try:
        # 1. –ß–∏—Ç–∞–µ–º –≤–µ—Å—å —Ñ–∞–π–ª –≤ —Å—Ç—Ä–æ–∫–∏
        with open(filepath, 'r', encoding='cp1251') as f:
            lines = f.readlines()

        # 2. –ù–∞—Ö–æ–¥–∏–º, –≥–¥–µ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è –∑–∞–≥–æ–ª–æ–≤–∫–∏ (station_b) –∏ –≥–¥–µ –æ—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
        header_start_line = -1
        data_start_line = -1
        
        for i, line in enumerate(lines):
            # "–ö–æ–Ω–µ—á–Ω—ã–π –ø—É–Ω–∫—Ç –º–∞—Ä—à—Ä—É—Ç–∞"
            if "–ö–æ–Ω–µ—á–Ω—ã–π –ø—É–Ω–∫—Ç –º–∞—Ä—à—Ä—É—Ç–∞" in line and header_start_line == -1:
                header_start_line = i + 1 
            
            # "‚Ññ –ø/–ø"
            if "‚Ññ –ø/–ø" in line and "–ù–∞—á–∞–ª—å–Ω—ã–π –ø—É–Ω–∫—Ç –º–∞—Ä—à—Ä—É—Ç–∞" in line:
                data_start_line = i
                break
        
        if header_start_line == -1 or data_start_line == -1:
            log.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ '–ö–æ–Ω–µ—á–Ω—ã–π –ø—É–Ω–∫—Ç' –∏–ª–∏ '‚Ññ –ø/–ø' –≤ {filepath}.")
            return None

        # 3. –°–æ–±–∏—Ä–∞–µ–º –∫–∞—Ä—Ç—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (station_b)
        header_lines = lines[header_start_line:data_start_line]
        header_cols = {}
        
        for line in header_lines:
            cleaned_line = line.rstrip(',\n')
            cols = cleaned_line.split(',')
            
            for col_idx in range(2, len(cols)): 
                if col_idx not in header_cols:
                    header_cols[col_idx] = []
                
                cell_value = cols[col_idx].strip()
                if cell_value:
                    header_cols[col_idx].append(cell_value)
        
        header_map = {}
        col_count = 1
        for col_idx in sorted(header_cols.keys()):
            full_name = " ".join(header_cols[col_idx])
            full_name = re.sub(r'\s+', ' ', full_name).strip()
            if full_name:
                header_map[str(col_count)] = full_name
                col_count += 1
                
        if not header_map:
             log.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –∫–∞—Ä—Ç—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (station_b) –∏–∑ {filepath}.")
             return None
        
        log.info(f"–°–æ–±—Ä–∞–Ω–∞ –∫–∞—Ä—Ç–∞ –∏–∑ {len(header_map)} –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (station_b).")

        # 4. –ß–∏—Ç–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ç–∞–±–ª–∏—Ü—É (–Ω–∞—á–∏–Ω–∞—è —Å "‚Ññ –ø/–ø")
        data_csv_lines = lines[data_start_line:]
        
        # –£–¥–∞–ª—è–µ–º –º—É—Å–æ—Ä–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ (–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞)
        if len(data_csv_lines) > 3:
             # –ò–Ω–¥–µ–∫—Å—ã 1 –∏ 2 –≤ data_csv_lines (—Ç.–µ. —Å—Ç—Ä–æ–∫–∏ 643 –∏ 644 –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
             del data_csv_lines[1:3] 
        
        data_io = StringIO("".join(data_csv_lines))

        df = pd.read_csv(
            data_io, 
            header=0, 
            encoding='cp1251'
        )

        # 5. –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –¥–≤–µ –∫–æ–ª–æ–Ω–∫–∏
        df.rename(columns={
            df.columns[0]: 'num_pp',
            df.columns[1]: 'station_a'
        }, inplace=True)

        # --- –ù–û–í–´–ô –®–ê–ì 5: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Å—Ç–∞–Ω—Ü–∏–∏ ---
        
        log.info("–ù–∞—á–∏–Ω–∞—é –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–∞–Ω—Ü–∏–π...")
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –ø—É—Å—Ç—ã–µ —è—á–µ–π–∫–∏ (–∫–æ—Ç–æ—Ä—ã–µ –Ω–µ NaN, –∞ –ø—Ä–æ—Å—Ç–æ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏) None
        df = df.replace({np.nan: None})
        
        rows_to_drop = []
        # –ò—Ç–µ—Ä–∏—Ä—É–µ–º —Å –∫–æ–Ω—Ü–∞, —á—Ç–æ–±—ã –æ–±—ä–µ–¥–∏–Ω—è—Ç—å "–≤–≤–µ—Ä—Ö"
        for i in range(len(df) - 1, 0, -1):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø—É—Å—Ç–∞ –ª–∏ –∫–æ–ª–æ–Ω–∫–∞ 'num_pp' (—ç—Ç–æ –ø—Ä–∏–∑–Ω–∞–∫ –ø–µ—Ä–µ–Ω–æ—Å–∞)
            if df.iloc[i]['num_pp'] is None:
                # –ë–µ—Ä–µ–º —Ç–µ–∫—É—â–µ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞–Ω—Ü–∏–∏ (–ø–µ—Ä–µ–Ω–æ—Å)
                current_station_part = str(df.iloc[i]['station_a']).strip()
                
                # –ë–µ—Ä–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞–Ω—Ü–∏–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Å—Ç—Ä–æ–∫–∏ (–≥–¥–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–æ–º–µ—Ä)
                prev_station_name = str(df.iloc[i-1]['station_a']).strip()
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º: –ø–æ–ª–Ω–æ–µ –∏–º—è + –ø—Ä–æ–±–µ–ª + —á–∞—Å—Ç—å –ø–µ—Ä–µ–Ω–æ—Å–∞
                new_station_name = f"{prev_station_name} {current_station_part}".strip()
                
                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É —Å –Ω–æ–º–µ—Ä–æ–º (i-1)
                df.iloc[i-1, df.columns.get_loc('station_a')] = new_station_name
                
                # –û—Ç–º–µ—á–∞–µ–º —Å—Ç—Ä–æ–∫—É –ø–µ—Ä–µ–Ω–æ—Å–∞ (i) –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
                rows_to_drop.append(i)

        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –ø–µ—Ä–µ–Ω–æ—Å–∞
        df.drop(df.index[rows_to_drop], inplace=True)
        log.info(f"–û–±—ä–µ–¥–∏–Ω–µ–Ω–æ –∏ —É–¥–∞–ª–µ–Ω–æ {len(rows_to_drop)} —Å—Ç—Ä–æ–∫-–ø–µ—Ä–µ–Ω–æ—Å–æ–≤.")
        
        # –û—á–∏—â–∞–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –Ω–æ–º–µ—Ä–∞–º–∏ (–¥–ª—è –ø–æ—Ä—è–¥–∫–∞, —Ç–µ–ø–µ—Ä—å –æ–Ω–∞ –Ω–µ –Ω—É–∂–Ω–∞)
        df.dropna(subset=['station_a'], inplace=True)
        df.reset_index(drop=True, inplace=True)
        
        # --- –ö–û–ù–ï–¶ –ù–û–í–û–ì–û –®–ê–ì–ê 5 ---

        # 6. "–ü–ª–∞–≤–∏–º" (melt) DataFrame
        col_station_b_numeric = [col for col in df.columns if col not in ['num_pp', 'station_a']]
        
        df_long = df.melt(
            id_vars=['station_a'], 
            value_vars=col_station_b_numeric, 
            var_name='station_b_num', 
            value_name='distance'
        )
        
        # 7. –û—á–∏—Å—Ç–∫–∞
        df_long['station_a'] = df_long['station_a'].astype(str).str.strip()
        df_long['station_b_num'] = df_long['station_b_num'].astype(str).str.strip()
        
        # 8. –û—á–∏—â–∞–µ–º –æ—Ç –Ω–µ—á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ int
        df_long = df_long[pd.to_numeric(df_long['distance'], errors='coerce').notna()]
        df_long['distance'] = df_long['distance'].astype(int)
        
        # 9. –£–¥–∞–ª—è–µ–º –º–∞—Ä—à—Ä—É—Ç—ã —Å 0 –∫–º
        df_long = df_long[df_long['distance'] > 0]
        
        # 10. *** –ì–õ–ê–í–ù–´–ô –§–ò–ö–°: –ó–∞–º–µ–Ω—è–µ–º '1', '2' –Ω–∞ –∏–º–µ–Ω–∞ ***
        df_long['station_b'] = df_long['station_b_num'].map(header_map)
        
        # 11. –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –∑–∞–º–µ–Ω–∏–ª–æ—Å—å
        if df_long['station_b'].isnull().any():
            missing_keys = df_long[df_long['station_b'].isnull()]['station_b_num'].unique()
            log.warning(f"‚ö†Ô∏è –í {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∏–º–µ–Ω–∞ –¥–ª—è station_b –∫–ª—é—á–µ–π: {missing_keys[:10]}...")
            df_long.dropna(subset=['station_b'], inplace=True)

        # 12. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –Ω–µ–Ω—É–∂–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü
        df_long = df_long[['station_a', 'station_b', 'distance']]
        df_long.drop_duplicates(subset=['station_a', 'station_b'], keep='first', inplace=True)
        
        log.info(f"‚úÖ –ú–∞—Ç—Ä–∏—Ü–∞ {os.path.basename(filepath)} –∑–∞–≥—Ä—É–∂–µ–Ω–∞, {len(df_long)} –£–ù–ò–ö–ê–õ–¨–ù–´–• –º–∞—Ä—à—Ä—É—Ç–æ–≤.")
        return df_long
        
    except FileNotFoundError:
        log.error(f"‚ùå –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª '{filepath}'.")
        return None
    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –º–∞—Ç—Ä–∏—Ü—ã {filepath}: {e}", exc_info=True)
        return None

# --- 4. –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –º–∏–≥—Ä–∞—Ü–∏–∏ ---

async def main_migrate():
    '''
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è. –ü–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è, —Å–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã, –∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ.
    '''
    if not TARIFF_DB_URL:
        log.error("‚ùå TARIFF_DATABASE_URL –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ .env —Ñ–∞–π–ª–µ. –ú–∏–≥—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞.")
        return
        
    log.info(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –Ω–æ–≤–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —Ç–∞—Ä–∏—Ñ–æ–≤: {TARIFF_DB_URL.split('@')[-1]}")
    
    # 1. –°–æ–∑–¥–∞–µ–º –¥–≤–∏–∂–æ–∫ –∏ —Ç–∞–±–ª–∏—Ü—ã
    engine = create_async_engine(TARIFF_DB_URL)
    async with engine.begin() as conn:
        log.info("–û—á–∏—Å—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–∞–±–ª–∏—Ü (–µ—Å–ª–∏ –µ—Å—Ç—å)...")
        await conn.run_sync(Base.metadata.drop_all)
        log.info("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ç–∞–±–ª–∏—Ü (tariff_stations, tariff_matrix)...")
        await conn.run_sync(Base.metadata.create_all)
    
    Session = async_sessionmaker(engine, expire_on_commit=False)
    
    # –ò—â–µ–º –ø–∞–ø–∫—É —Å –¥–∞–Ω–Ω—ã–º–∏
    data_dir_path = os.path.join(project_root_dir, 'zdtarif_bot', 'data')
    if not os.path.exists(data_dir_path):
        data_dir_path = os.path.join(project_root_dir, 'data')
        if not os.path.exists(data_dir_path):
             log.error(f"‚ùå –ù–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –ø–∞–ø–∫—É 'data' –∏–ª–∏ 'zdtarif_bot/data' –≤ {project_root_dir}")
             return
    
    log.info(f"–ò—Å–ø–æ–ª—å–∑—É—é –ø–∞–ø–∫—É —Å –¥–∞–Ω–Ω—ã–º–∏: {data_dir_path}")

    # 2. –ú–∏–≥—Ä–∞—Ü–∏—è —Å—Ç–∞–Ω—Ü–∏–π (—Ç–æ–ª—å–∫–æ 2-–†–ü*.csv)
    log.info("--- 1/2: –ù–∞—á–∏–Ω–∞—é –º–∏–≥—Ä–∞—Ü–∏—é –°—Ç–∞–Ω—Ü–∏–π (—Ç–æ–ª—å–∫–æ 2-–†–ü*.csv) ---")
    
    station_files = glob.glob(os.path.join(data_dir_path, '2-–†–ü*.csv'))
    log.info(f"–ù–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã —Å—Ç–∞–Ω—Ü–∏–π (2-–†–ü): {[os.path.basename(f) for f in station_files]}")
    
    all_stations_dfs = []
    for filepath in station_files:
        df = load_kniga_2_rp(filepath)
        if df is not None:
            all_stations_dfs.append(df)

    if not all_stations_dfs:
        log.error("‚ùå –ù–∏ –æ–¥–∏–Ω —Ñ–∞–π–ª —Å—Ç–∞–Ω—Ü–∏–π (2-–†–ü*.csv) –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω. –ú–∏–≥—Ä–∞—Ü–∏—è —Å—Ç–∞–Ω—Ü–∏–π –ø—Ä–æ–≤–∞–ª–µ–Ω–∞.")
        return
        
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ DF –∏ —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    stations_df = pd.concat(all_stations_dfs, ignore_index=True)
    stations_df.drop_duplicates(subset=['station_code'], keep='first', inplace=True)
    
    log.info(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(stations_df)} –£–ù–ò–ö–ê–õ–¨–ù–´–• —Å—Ç–∞–Ω—Ü–∏–π –≤–æ –≤—Å–µ—Ö —Ñ–∞–π–ª–∞—Ö.")
    
    stations_df = stations_df.where(pd.notnull(stations_df), None)

    async with Session() as session:
        async with session.begin():
            stations_to_add = []
            for _, row in stations_df.iterrows():
                stations_to_add.append(
                    TariffStation(
                        name=row['station_name'],
                        code=row['station_code'],
                        railway=row['railway'],
                        operations=row['operations'],
                        transit_points=parse_transit_points_for_db(row['transit_points_raw'])
                    )
                )
            log.info(f"–î–æ–±–∞–≤–ª—è—é {len(stations_to_add)} —Å—Ç–∞–Ω—Ü–∏–π –≤ –±–∞–∑—É...")
            session.add_all(stations_to_add)
        await session.commit()
    log.info("‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è —Å—Ç–∞–Ω—Ü–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")


    # --- 3. –ú–∏–≥—Ä–∞—Ü–∏—è (–í–°–ï 3-*.csv, –ö–†–û–ú–ï "–ø–æ–ª–æ–∂–µ–Ω–∏–π") ---
    log.info("--- 2/2: –ù–∞—á–∏–Ω–∞—é –º–∏–≥—Ä–∞—Ü–∏—é –ú–∞—Ç—Ä–∏—Ü (–≤—Å–µ 3-*.csv) ---")
    
    # 1. –ù–∞—Ö–æ–¥–∏–º –ê–ë–°–û–õ–Æ–¢–ù–û –í–°–ï —Ñ–∞–π–ª—ã 3-*.csv
    all_matrix_files = glob.glob(os.path.join(data_dir_path, '3-*.csv'))
    
    # 2. üêû –ù–û–í–´–ô –§–ò–õ–¨–¢–†: –ò—Å–∫–ª—é—á–∞–µ–º —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –¢–û–ß–ù–û –Ω–µ —è–≤–ª—è—é—Ç—Å—è –º–∞—Ç—Ä–∏—Ü–∞–º–∏
    files_to_exclude = [
        '3-–í–≤–æ–¥–Ω—ã–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è.csv',
        '3-–û–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è.csv'
    ]
    
    # –°–æ–∑–¥–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    matrix_files_to_process = []
    for f_path in all_matrix_files:
        f_name = os.path.basename(f_path)
        if f_name not in files_to_exclude:
            matrix_files_to_process.append(f_path)
        else:
            log.warning(f"–§–∞–π–ª {f_name} –∏—Å–∫–ª—é—á–µ–Ω –∏–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏, —Ç.–∫. –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –º–∞—Ç—Ä–∏—Ü–µ–π.")
            
    log.info(f"–ù–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã –º–∞—Ç—Ä–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {[os.path.basename(f) for f in matrix_files_to_process]}")


    total_routes_added = 0
    
    async with Session() as session:
        # 3. üêû –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
        for filepath in matrix_files_to_process: 
                
            log.info(f"--- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {os.path.basename(filepath)} ---")
            matrix_df = load_kniga_3_matrix(filepath)
            
            if matrix_df is not None and not matrix_df.empty:
                async with session.begin():
                    log.info(f"–î–æ–±–∞–≤–ª—è—é {len(matrix_df)} –º–∞—Ä—à—Ä—É—Ç–æ–≤ (—Å –ø—Ä–æ–ø—É—Å–∫–æ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (ON CONFLICT DO NOTHING))...")
                    try:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º "upsert" (ON CONFLICT DO NOTHING)
                        for record in matrix_df.to_dict(orient='records'):
                            stmt = pg_insert(TariffMatrix).values(**record).on_conflict_do_nothing(
                                index_elements=['station_a', 'station_b']
                            )
                            await session.execute(stmt)
                        
                        total_routes_added += len(matrix_df) 
                        
                    except Exception as e:
                        log.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤–∫–µ {os.path.basename(filepath)}: {e}", exc_info=True)
                        await session.rollback()
                await session.commit()
            else:
                log.warning(f"–§–∞–π–ª {os.path.basename(filepath)} –ø—Ä–æ–ø—É—â–µ–Ω (–ø—É—Å—Ç–æ–π –∏–ª–∏ –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏).")

    log.info(f"‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –í—Å–µ–≥–æ –ø–æ–ø—ã—Ç–æ–∫ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –º–∞—Ä—à—Ä—É—Ç–æ–≤: {total_routes_added}")

    log.info("üéâüéâüéâ == –ú–ò–ì–†–ê–¶–ò–Ø –¢–ê–†–ò–§–ù–û–ô –ë–ê–ó–´ –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–ê! ==")
    
    await engine.dispose()


if __name__ == "__main__":
    env_path = os.path.join(project_root_dir, '.env')
    if os.path.exists(env_path):
        log.info(f"–ó–∞–≥—Ä—É–∂–∞—é .env –∏–∑ {env_path}")
        load_dotenv(dotenv_path=env_path)
    else:
        log.warning(f"–§–∞–π–ª .env –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {project_root_dir}, –∏—Å–ø–æ–ª—å–∑—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã.")
        
    TARIFF_DB_URL = os.getenv("TARIFF_DATABASE_URL")
    
    asyncio.run(main_migrate())