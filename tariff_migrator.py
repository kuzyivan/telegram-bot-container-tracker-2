# tariff_migrator.py
import asyncio
import os
import re
import pandas as pd
import numpy as np
import sys
import glob
from dotenv import load_dotenv
from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column
from sqlalchemy import String, Integer, ARRAY, Index, UniqueConstraint
from sqlalchemy.dialects.postgresql import insert as pg_insert
import logging
from io import StringIO 

# --- 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ .env ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
log = logging.getLogger(__name__)

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
current_file_path = os.path.abspath(__file__)
project_root_dir = os.path.dirname(current_file_path)
sys.path.insert(0, project_root_dir)

load_dotenv()
TARIFF_DB_URL = os.getenv("TARIFF_DATABASE_URL")

# --- 2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ ORM –ú–æ–¥–µ–ª–µ–π ---

class Base(DeclarativeBase):
    pass

class TariffStation(Base):
    __tablename__ = 'tariff_stations'
    id: Mapped[int] = mapped_column(primary_key=True)
    name: Mapped[str] = mapped_column(String, index=True) 
    code: Mapped[str] = mapped_column(String(6), index=True, unique=True) 
    railway: Mapped[str | None] = mapped_column(String)
    operations: Mapped[str | None] = mapped_column(String)
    transit_points: Mapped[list[str] | None] = mapped_column(ARRAY(String)) 

    __table_args__ = (
        Index('ix_tariff_stations_name_code', 'name', 'code'),
    )

class TariffMatrix(Base):
    __tablename__ = 'tariff_matrix'
    id: Mapped[int] = mapped_column(primary_key=True)
    station_a: Mapped[str] = mapped_column(String, index=True)
    station_b: Mapped[str] = mapped_column(String, index=True)
    distance: Mapped[int] = mapped_column(Integer)

    __table_args__ = (
        UniqueConstraint('station_a', 'station_b', name='uq_station_pair'),
    )

# --- 3. –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---

def parse_transit_points_for_db(tp_string: str) -> list[str]:
    if not isinstance(tp_string, str) or not tp_string:
        return []
    pattern = re.compile(r'(\d{6})\s(.*?)\s-\s(\d+)–∫–º')
    matches = pattern.findall(tp_string)
    transit_points_str = []
    for match in matches:
        transit_points_str.append(f"{match[0]}:{match[1].strip()}:{int(match[2])}")
    return transit_points_str

def load_kniga_2_rp(filepath: str) -> pd.DataFrame | None:
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –Ω–∞—á–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–ø—É—Å–∫–∞—è —à–∞–ø–∫—É
        # –û–±—ã—á–Ω–æ —à–∞–ø–∫–∞ –∑–∞–Ω–∏–º–∞–µ—Ç 5-7 —Å—Ç—Ä–æ–∫, –∏—â–µ–º —Å—Ç—Ä–æ–∫—É –≥–¥–µ –µ—Å—Ç—å —Ü–∏—Ñ—Ä—ã –≤ –ø–µ—Ä–≤–æ–π –∫–æ–ª–æ–Ω–∫–µ
        with open(filepath, 'r', encoding='cp1251') as f:
            lines = f.readlines()
        
        start_row = 0
        for i, line in enumerate(lines[:20]):
            if "–ö–æ–¥ —Å—Ç–∞–Ω—Ü–∏–∏" in line or "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ" in line:
                start_row = i + 1
                break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏, –±–µ—Ä–µ–º —Ö–∞—Ä–¥–∫–æ–¥ 6
        if start_row == 0: start_row = 6

        df = pd.read_csv(
            filepath,
            skiprows=start_row,
            names=['num', 'station_name', 'operations', 'railway', 'transit_points_raw', 'station_code'],
            encoding='cp1251',
            dtype={'station_code': str},
            on_bad_lines='skip' # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –±–∏—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        )
        df['station_name'] = df['station_name'].str.strip()
        df['station_code'] = df['station_code'].str.strip()
        
        df.dropna(subset=['station_name', 'station_code'], inplace=True)
        df.drop_duplicates(subset=['station_code'], keep='first', inplace=True)
        
        log.info(f"‚úÖ –§–∞–π–ª —Å—Ç–∞–Ω—Ü–∏–π {os.path.basename(filepath)}: –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π.")
        return df
    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç–∞–Ω—Ü–∏–π {filepath}: {e}", exc_info=True)
        return None

def load_kniga_3_matrix(filepath: str) -> pd.DataFrame | None:
    '''
    –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–∞—Ç—Ä–∏—Ü—ã.
    '''
    try:
        with open(filepath, 'r', encoding='cp1251') as f:
            lines = f.readlines()

        # 1. –ü–æ–∏—Å–∫ –≥—Ä–∞–Ω–∏—Ü –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ –¥–∞–Ω–Ω—ã—Ö
        header_start_line = -1
        data_start_line = -1
        
        for i, line in enumerate(lines):
            if "–ö–æ–Ω–µ—á–Ω—ã–π –ø—É–Ω–∫—Ç –º–∞—Ä—à—Ä—É—Ç–∞" in line and header_start_line == -1:
                header_start_line = i + 1 
            # –ò—â–µ–º —Å—Ç—Ä–æ–∫—É, –≥–¥–µ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ. –û–±—ã—á–Ω–æ –æ–Ω–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç "‚Ññ –ø/–ø"
            if "‚Ññ –ø/–ø" in line and "–ù–∞—á–∞–ª—å–Ω—ã–π –ø—É–Ω–∫—Ç" in line:
                data_start_line = i
                break
        
        if header_start_line == -1 or data_start_line == -1:
            log.error(f"‚ö†Ô∏è –°—Ç—Ä–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ {filepath}. –ù–µ –Ω–∞–π–¥–µ–Ω—ã –º–∞—Ä–∫–µ—Ä—ã –Ω–∞—á–∞–ª–∞.")
            return None

        # 2. –ü–∞—Ä—Å–∏–Ω–≥ –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–∞–Ω—Ü–∏–π –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è (–ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫)
        # –û–Ω–∏ —Ä–∞–∑–±—Ä–æ—Å–∞–Ω—ã –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ –Ω–∞–¥ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ–π
        header_lines = lines[header_start_line:data_start_line]
        header_cols = {}
        
        for line in header_lines:
            cleaned_line = line.rstrip(',\n')
            cols = cleaned_line.split(',')
            # –ü–µ—Ä–≤—ã–µ 2 –∫–æ–ª–æ–Ω–∫–∏ - —ç—Ç–æ –æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            for col_idx in range(2, len(cols)): 
                val = cols[col_idx].strip()
                if val:
                    if col_idx not in header_cols: header_cols[col_idx] = []
                    header_cols[col_idx].append(val)
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–∞—Ä—Ç—É { "–ù–æ–º–µ—Ä –∫–æ–ª–æ–Ω–∫–∏ –≤ CSV": "–ü–æ–ª–Ω–æ–µ –∏–º—è —Å—Ç–∞–Ω—Ü–∏–∏" }
        # –í–∞–∂–Ω–æ: Pandas read_csv –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ DATA SECTION –¥–∞—Å—Ç –∫–æ–ª–æ–Ω–∫–∞–º –∏–º–µ–Ω–∞ '1', '2', '3' –∏ —Ç.–¥.
        # –ù–∞–º –Ω—É–∂–Ω–æ —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –ø–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä –∫–æ–ª–æ–Ω–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –∏–º–µ–Ω–µ–º.
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—É—é –∫–æ–ª–æ–Ω–∫—É —Å –¥–∞–Ω–Ω—ã–º–∏ –≤ —Å—Ç—Ä–æ–∫–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        # –°—Ç—Ä–æ–∫–∞ data_start_line –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫: "‚Ññ –ø/–ø,–ù–∞—á.–ø—É–Ω–∫—Ç,1,2,3,4..."
        data_header_row = lines[data_start_line].strip().split(',')
        
        # –ö–∞—Ä—Ç–∞: –∫–ª—é—á - –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ –≤ DataFrame ('1', '2'...), –∑–Ω–∞—á–µ–Ω–∏–µ - –ò–º—è —Å—Ç–∞–Ω—Ü–∏–∏
        column_name_to_station_map = {}
        
        # –ò–Ω–¥–µ–∫—Å—ã –≤ data_header_row —Å–¥–≤–∏–Ω—É—Ç—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ header_cols –Ω–∞ —Ç–æ –∂–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        for col_idx, station_name_parts in header_cols.items():
            if col_idx < len(data_header_row):
                col_name_in_df = data_header_row[col_idx].strip() # –≠—Ç–æ –±—É–¥–µ—Ç '1', '2', '5' –∏ —Ç.–¥.
                full_name = " ".join(station_name_parts)
                full_name = re.sub(r'\s+', ' ', full_name).strip()
                column_name_to_station_map[col_name_in_df] = full_name

        if not column_name_to_station_map:
             log.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –∫–∞—Ä—Ç—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è {filepath}.")
             return None
        
        log.info(f"–§–∞–π–ª {os.path.basename(filepath)}: –Ω–∞–π–¥–µ–Ω–æ {len(column_name_to_station_map)} —Ü–µ–ª–µ–≤—ã—Ö —Å—Ç–∞–Ω—Ü–∏–π.")

        # 3. –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        # –°—á–∏—Ç—ã–≤–∞–µ–º –≤—Å—ë, –Ω–∞—á–∏–Ω–∞—è —Å–æ —Å—Ç—Ä–æ–∫–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        data_io = StringIO("".join(lines[data_start_line:]))
        
        df = pd.read_csv(data_io, header=0, encoding='cp1251', on_bad_lines='skip')
        
        # –ü–µ—Ä–µ–∏–º–µ–Ω—É–µ–º –ø–µ—Ä–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        df.rename(columns={df.columns[0]: 'num_pp', df.columns[1]: 'station_a'}, inplace=True)

        # 4. "–°–∫–ª–µ–∏–≤–∞–Ω–∏–µ" —Ä–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–∞–Ω—Ü–∏–π –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏—è (–≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã—Ö)
        # –ï—Å–ª–∏ num_pp –ø—É—Å—Ç–æ–π, –∑–Ω–∞—á–∏—Ç —ç—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Å—Ç–∞–Ω—Ü–∏–∏
        df = df.replace({np.nan: None})
        rows_to_drop = []
        
        for i in range(len(df)):
            # –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if i == 0: continue
            
            curr_num = df.iloc[i]['num_pp']
            curr_name = str(df.iloc[i]['station_a'] or '').strip()
            
            # –ï—Å–ª–∏ –Ω–µ—Ç –Ω–æ–º–µ—Ä–∞, –Ω–æ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç –≤ station_a - —ç—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–π
            if not curr_num and curr_name:
                prev_idx = i - 1
                # –ò—â–µ–º "—Ä–æ–¥–∏—Ç–µ–ª—è" –≤—ã—à–µ
                while prev_idx in rows_to_drop and prev_idx >= 0:
                    prev_idx -= 1
                
                if prev_idx >= 0:
                    prev_name = str(df.iloc[prev_idx]['station_a']).strip()
                    df.iloc[prev_idx, 1] = f"{prev_name} {curr_name}".strip()
                    rows_to_drop.append(i)
            # –ï—Å–ª–∏ –∏ –Ω–æ–º–µ—Ä–∞ –Ω–µ—Ç, –∏ –∏–º–µ–Ω–∏ –Ω–µ—Ç - –º—É—Å–æ—Ä
            elif not curr_num and not curr_name:
                rows_to_drop.append(i)

        df.drop(df.index[rows_to_drop], inplace=True)
        
        # 5. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –¥–ª–∏–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (Melt)
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ –Ω–∞—à–µ–π –∫–∞—Ä—Ç–µ —Å—Ç–∞–Ω—Ü–∏–π
        valid_value_vars = [c for c in df.columns if c in column_name_to_station_map]
        
        df_long = df.melt(
            id_vars=['station_a'], 
            value_vars=valid_value_vars, 
            var_name='station_b_key', 
            value_name='distance'
        )
        
        # 6. –û—á–∏—Å—Ç–∫–∞ –∏ –º–∞–ø–ø–∏–Ω–≥
        df_long = df_long[pd.to_numeric(df_long['distance'], errors='coerce').notna()]
        df_long['distance'] = df_long['distance'].astype(int)
        df_long = df_long[df_long['distance'] > 0]
        
        df_long['station_a'] = df_long['station_a'].astype(str).str.strip()
        # –ú–∞–ø–∏–º –∫–æ–¥ –∫–æ–ª–æ–Ω–∫–∏ ('1', '2') –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–µ –∏–º—è ('–ú–æ—Å–∫–≤–∞...')
        df_long['station_b'] = df_long['station_b_key'].map(column_name_to_station_map)
        
        df_long.dropna(subset=['station_b', 'station_a'], inplace=True)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–µ–µ
        final_df = df_long[['station_a', 'station_b', 'distance']].copy()
        
        log.info(f"‚úÖ {os.path.basename(filepath)} –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {len(final_df)} –º–∞—Ä—à—Ä—É—Ç–æ–≤.")
        return final_df
        
    except Exception as e:
        log.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ {filepath}: {e}", exc_info=True)
        return None

# --- 4. –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ ---

async def main_migrate():
    if not TARIFF_DB_URL:
        log.error("‚ùå TARIFF_DATABASE_URL –Ω–µ –∑–∞–¥–∞–Ω.")
        return
        
    engine = create_async_engine(TARIFF_DB_URL)
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all) # –°–æ–∑–¥–∞–µ–º –µ—Å–ª–∏ –Ω–µ—Ç (–ª—É—á—à–µ –¥—Ä–æ–ø–Ω—É—Ç—å –≤—Ä—É—á–Ω—É—é –µ—Å–ª–∏ –Ω–∞–¥–æ —á–∏—Å—Ç—É—é)
        # –ï—Å–ª–∏ –Ω—É–∂–Ω–∞ –ø–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π:
        log.info("–û—á–∏—Å—Ç–∫–∞ —Ç–∞–±–ª–∏—Ü –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π...")
        await conn.run_sync(Base.metadata.drop_all)
        await conn.run_sync(Base.metadata.create_all)
    
    Session = async_sessionmaker(engine, expire_on_commit=False)
    
    # –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
    data_dir = os.path.join(project_root_dir, 'zdtarif_bot', 'data')
    if not os.path.exists(data_dir):
        data_dir = os.path.join(project_root_dir, 'data') # Fallback

    # 1. –°—Ç–∞–Ω—Ü–∏–∏
    log.info("--- –ó–∞–≥—Ä—É–∑–∫–∞ –°—Ç–∞–Ω—Ü–∏–π ---")
    station_files = glob.glob(os.path.join(data_dir, '2-–†–ü*.csv'))
    all_stations = []
    for f in station_files:
        df = load_kniga_2_rp(f)
        if df is not None: all_stations.append(df)
    
    if all_stations:
        full_stations = pd.concat(all_stations).drop_duplicates(subset=['station_code'])
        async with Session() as session:
            # Batch insert
            batch_size = 5000
            total = len(full_stations)
            for start in range(0, total, batch_size):
                end = min(start + batch_size, total)
                batch = full_stations.iloc[start:end]
                values = []
                for _, row in batch.iterrows():
                    values.append({
                        'name': row['station_name'],
                        'code': row['station_code'],
                        'railway': row['railway'],
                        'operations': row['operations'],
                        'transit_points': parse_transit_points_for_db(row['transit_points_raw'])
                    })
                await session.execute(pg_insert(TariffStation).values(values).on_conflict_do_nothing())
                await session.commit()
                log.info(f"–°—Ç–∞–Ω—Ü–∏–∏: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {end}/{total}")

    # 2. –ú–∞—Ç—Ä–∏—Ü—ã
    log.info("--- –ó–∞–≥—Ä—É–∑–∫–∞ –ú–∞—Ç—Ä–∏—Ü ---")
    matrix_files = glob.glob(os.path.join(data_dir, '3-*.csv'))
    # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ-–º–∞—Ç—Ä–∏—Ü—ã
    matrix_files = [f for f in matrix_files if "–í–≤–æ–¥–Ω—ã–µ" not in f and "–û–±—â–∏–µ" not in f]
    
    combined_dfs = []
    for f in matrix_files:
        df = load_kniga_3_matrix(f)
        if df is not None: combined_dfs.append(df)
    
    if not combined_dfs:
        log.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏.")
        return

    full_matrix = pd.concat(combined_dfs, ignore_index=True)
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–µ –º–∞—Ä—à—Ä—É—Ç—ã (B -> A)
    log.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—Ä–∞—Ç–Ω—ã—Ö –º–∞—Ä—à—Ä—É—Ç–æ–≤...")
    reversed_matrix = full_matrix.rename(columns={'station_a': 'station_b', 'station_b': 'station_a'})
    full_matrix = pd.concat([full_matrix, reversed_matrix], ignore_index=True)
    
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    full_matrix.drop_duplicates(subset=['station_a', 'station_b'], inplace=True)
    
    total_routes = len(full_matrix)
    log.info(f"–í—Å–µ–≥–æ –º–∞—Ä—à—Ä—É—Ç–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏: {total_routes}")
    
    async with Session() as session:
        batch_size = 5000 # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–∞–∑–º–µ—Ä
        for start in range(0, total_routes, batch_size):
            end = min(start + batch_size, total_routes)
            batch = full_matrix.iloc[start:end]
            records = batch.to_dict(orient='records')
            
            stmt = pg_insert(TariffMatrix).values(records).on_conflict_do_nothing(
                index_elements=['station_a', 'station_b']
            )
            await session.execute(stmt)
            await session.commit()
            if start % 50000 == 0:
                log.info(f"–ú–∞—Ç—Ä–∏—Ü–∞: –∑–∞–≥—Ä—É–∂–µ–Ω–æ {end}/{total_routes}")

    log.info("üéâ –ú–∏–≥—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    await engine.dispose()

if __name__ == "__main__":
    asyncio.run(main_migrate())