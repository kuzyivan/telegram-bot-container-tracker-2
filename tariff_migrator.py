# tariff_migrator.py
import asyncio
import os
import re
import pandas as pd
import sys
import glob
from dotenv import load_dotenv
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column
from sqlalchemy import String, Integer, ARRAY, Index, UniqueConstraint
from sqlalchemy.exc import IntegrityError
from sqlalchemy.dialects.postgresql import insert as pg_insert
import logging
from io import StringIO # üêû –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è —á—Ç–µ–Ω–∏—è —Å—Ç—Ä–æ–∫ –∫–∞–∫ —Ñ–∞–π–ª–∞

# --- 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ .env ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
log = logging.getLogger(__name__)

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ zdtarif_bot/data
current_file_path = os.path.abspath(__file__)
project_root_dir = os.path.dirname(current_file_path)
sys.path.insert(0, project_root_dir)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è (–æ—Å–æ–±–µ–Ω–Ω–æ TARIFF_DATABASE_URL)
load_dotenv()
TARIFF_DB_URL = os.getenv("TARIFF_DATABASE_URL")

# --- 2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ ORM –ú–æ–¥–µ–ª–µ–π –¥–ª—è –Ω–æ–≤–æ–π –ë–î ---

class Base(DeclarativeBase):
    pass

class TariffStation(Base):
    '''
    –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ 2-–†–ü.csv.
    '''
    __tablename__ = 'tariff_stations'
    id: Mapped[int] = mapped_column(primary_key=True)
    
    # --- üêû –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: name –ù–ï —É–Ω–∏–∫–∞–ª—å–Ω–æ ---
    name: Mapped[str] = mapped_column(String, index=True) 
    # --- üèÅ –ö–û–ù–ï–¶ –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø üèÅ ---
    
    # --- üêû –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: code –£–ù–ò–ö–ê–õ–ï–ù ---
    code: Mapped[str] = mapped_column(String(6), index=True, unique=True) 
    # --- üèÅ –ö–û–ù–ï–¶ –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø üèÅ ---

    railway: Mapped[str | None] = mapped_column(String)
    operations: Mapped[str | None] = mapped_column(String)
    transit_points: Mapped[list[str] | None] = mapped_column(ARRAY(String)) 

    __table_args__ = (
        Index('ix_tariff_stations_name_code', 'name', 'code'),
    )

class TariffMatrix(Base):
    '''
    –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ 3-*.csv.
    '''
    __tablename__ = 'tariff_matrix'
    id: Mapped[int] = mapped_column(primary_key=True)
    
    station_a: Mapped[str] = mapped_column(String, index=True)
    station_b: Mapped[str] = mapped_column(String, index=True)
    distance: Mapped[int] = mapped_column(Integer)

    __table_args__ = (
        UniqueConstraint('station_a', 'station_b', name='uq_station_pair'),
    )

# --- 3. –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ ---

def parse_transit_points_for_db(tp_string: str) -> list[str]:
    '''
    –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–æ–∫—É —Ç—Ä–∞–Ω–∑–∏—Ç–Ω—ã—Ö –ø—É–Ω–∫—Ç–æ–≤ –∏–∑ 2-–†–ü.csv –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫.
    '''
    if not isinstance(tp_string, str) or not tp_string:
        return []
    
    pattern = re.compile(r'(\d{6})\s(.*?)\s-\s(\d+)–∫–º')
    matches = pattern.findall(tp_string)
    
    transit_points_str = []
    for match in matches:
        transit_points_str.append(f"{match[0]}:{match[1].strip()}:{int(match[2])}")
        
    return transit_points_str

def load_kniga_2_rp(filepath: str) -> pd.DataFrame | None:
    '''
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç 2-–†–ü.csv –∏–∑ zdtarif_bot/data
    '''
    try:
        df = pd.read_csv(
            filepath,
            skiprows=6, # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
            names=[
                'num', 'station_name', 'operations', 'railway', 
                'transit_points_raw', 'station_code'
            ],
            encoding='cp1251',
            dtype={'station_code': str} 
        )
        df['station_name'] = df['station_name'].str.strip()
        df['station_code'] = df['station_code'].str.strip()
        df['railway'] = df['railway'].str.strip()
        df['operations'] = df['operations'].str.strip()

        df.dropna(subset=['station_name', 'station_code'], inplace=True)
        
        # --- üêû –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –ö–û–î–£, –∞ –Ω–µ –ø–æ –ò–ú–ï–ù–ò üêû ---
        df.drop_duplicates(subset=['station_code'], keep='first', inplace=True)
        # --- üèÅ –ö–û–ù–ï–¶ –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø üèÅ ---
        
        log.info(f"‚úÖ –§–∞–π–ª {os.path.basename(filepath)} –∑–∞–≥—Ä—É–∂–µ–Ω, {len(df)} –£–ù–ò–ö–ê–õ–¨–ù–´–• —Å—Ç–∞–Ω—Ü–∏–π (–ø–æ –∫–æ–¥—É).")
        return df
    except FileNotFoundError:
        log.error(f"‚ùå –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª '{filepath}'.")
        return None
    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {filepath}: {e}", exc_info=True)
        return None

# --- üêû –ù–ê–ß–ê–õ–û –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ô –§–£–ù–ö–¶–ò–ò üêû ---
def load_kniga_3_matrix(filepath: str) -> pd.DataFrame | None:
    '''
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É (3-*.csv) –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –µ–µ –≤ "–¥–ª–∏–Ω–Ω—ã–π" —Ñ–æ—Ä–º–∞—Ç,
    –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å—á–∏—Ç—ã–≤–∞—è –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏.
    '''
    try:
        # 1. –ß–∏—Ç–∞–µ–º –≤–µ—Å—å —Ñ–∞–π–ª –≤ —Å—Ç—Ä–æ–∫–∏
        with open(filepath, 'r', encoding='cp1251') as f:
            lines = f.readlines()

        # 2. –ù–∞—Ö–æ–¥–∏–º, –≥–¥–µ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è –∑–∞–≥–æ–ª–æ–≤–∫–∏ (station_b) –∏ –≥–¥–µ –æ—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
        header_start_line = -1
        data_start_line = -1
        
        for i, line in enumerate(lines):
            # "–ö–æ–Ω–µ—á–Ω—ã–π –ø—É–Ω–∫—Ç –º–∞—Ä—à—Ä—É—Ç–∞" (Source 13)
            if "–ö–æ–Ω–µ—á–Ω—ã–π –ø—É–Ω–∫—Ç –º–∞—Ä—à—Ä—É—Ç–∞" in line and header_start_line == -1:
                header_start_line = i + 1 # –ù–∞—á–∏–Ω–∞–µ–º —Å–æ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–æ–∫–∏ (Source 15)
            
            # "‚Ññ –ø/–ø" (Source 642)
            if "‚Ññ –ø/–ø" in line and "–ù–∞—á–∞–ª—å–Ω—ã–π –ø—É–Ω–∫—Ç –º–∞—Ä—à—Ä—É—Ç–∞" in line:
                data_start_line = i
                break
        
        if header_start_line == -1 or data_start_line == -1:
            log.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ '–ö–æ–Ω–µ—á–Ω—ã–π –ø—É–Ω–∫—Ç' –∏–ª–∏ '‚Ññ –ø/–ø' –≤ {filepath}.")
            return None

        # 3. –°–æ–±–∏—Ä–∞–µ–º –∫–∞—Ä—Ç—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (station_b)
        # –û–Ω–∏ –≤ —Å—Ç—Ä–æ–∫–∞—Ö —Å header_start_line –ø–æ data_start_line - 1
        # –§–æ—Ä–º–∞—Ç: ,,–ò–ú–Ø 1 (–∫–æ–¥), –ò–ú–Ø 2 (–∫–æ–¥), ...
        #         ,,(–∫–æ–¥), (–∫–æ–¥), ...
        #         ,,–¥–æ),,), ...
        # –≠—Ç–∏ —Å—Ç—Ä–æ–∫–∏ –Ω—É–∂–Ω–æ "—Å–∫–ª–µ–∏—Ç—å" –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º.
        
        header_lines = lines[header_start_line:data_start_line]
        
        # header_cols[2] = ["–ê–≤–¥–µ–µ–≤–∫–∞ (89", "89-—è", "–¥–æ)"]
        # header_cols[3] = ["–ê–≥—Ä—ã–∑ (24", "–ì–æ—Ä—å–∫)"]
        header_cols = {}
        
        for line in header_lines:
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –∑–∞–ø—è—Ç—ã–µ –≤ –∫–æ–Ω—Ü–µ, –µ—Å–ª–∏ –µ—Å—Ç—å
            cleaned_line = line.rstrip(',\n')
            cols = cleaned_line.split(',')
            
            # –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º —Å—Ç–æ–ª–±—Ü–æ–≤, –Ω–∞—á–∏–Ω–∞—è —Å 3-–≥–æ (–∏–Ω–¥–µ–∫—Å 2)
            for col_idx in range(2, len(cols)): 
                if col_idx not in header_cols:
                    header_cols[col_idx] = []
                
                cell_value = cols[col_idx].strip()
                if cell_value:
                    header_cols[col_idx].append(cell_value)
        
        # –¢–µ–ø–µ—Ä—å –æ–±—ä–µ–¥–∏–Ω—è–µ–º —è—á–µ–π–∫–∏ –≤ –ø–æ–ª–Ω—ã–µ –∏–º–µ–Ω–∞ –∏ –Ω—É–º–µ—Ä—É–µ–º –∏—Ö
        # header_map = {'1': '–ò–º—è 1', '2': '–ò–º—è 2', ...}
        header_map = {}
        col_count = 1
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–Ω–¥–µ–∫—Å—É —Å—Ç–æ–ª–±—Ü–∞ (col_idx), —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—Ä—è–¥–æ–∫
        for col_idx in sorted(header_cols.keys()):
            full_name = " ".join(header_cols[col_idx])
            # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
            full_name = re.sub(r'\s+', ' ', full_name).strip()
            if full_name:
                # –ö–ª—é—á - —ç—Ç–æ *–Ω–æ–º–µ—Ä —Å—Ç–æ–ª–±—Ü–∞*, –∫–∞–∫ –≤ —Å—Ç—Ä–æ–∫–µ [Source 642]
                header_map[str(col_count)] = full_name
                col_count += 1
                
        if not header_map:
             log.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –∫–∞—Ä—Ç—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (station_b) –∏–∑ {filepath}.")
             return None
        
        log.info(f"–°–æ–±—Ä–∞–Ω–∞ –∫–∞—Ä—Ç–∞ –∏–∑ {len(header_map)} –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (station_b).")

        # 4. –ß–∏—Ç–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ç–∞–±–ª–∏—Ü—É (–Ω–∞—á–∏–Ω–∞—è —Å "‚Ññ –ø/–ø")
        # –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º StringIO, —á—Ç–æ–±—ã –ø–µ—Ä–µ–¥–∞—Ç—å pandas —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
        data_csv_lines = lines[data_start_line:]
        
        # –í —Ñ–∞–π–ª–∞—Ö 3-1/3-2.csv —Å—Ç—Ä–æ–∫–∏ 643 –∏ 644 (–∏–Ω–¥–µ–∫—Å—ã 1 –∏ 2 –≤ data_csv_lines) 
        # —è–≤–ª—è—é—Ç—Å—è –º—É—Å–æ—Ä–Ω—ã–º–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞. –£–¥–∞–ª—è–µ–º –∏—Ö.
        if len(data_csv_lines) > 3:
             del data_csv_lines[1:3] 
        
        data_io = StringIO("".join(data_csv_lines))

        df = pd.read_csv(
            data_io, 
            header=0, # <-- "‚Ññ –ø/–ø"
            encoding='cp1251'
        )

        # 5. –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –¥–≤–µ –∫–æ–ª–æ–Ω–∫–∏
        df.rename(columns={
            df.columns[0]: 'num_pp',
            df.columns[1]: 'station_a'
        }, inplace=True)

        # 6. "–ü–ª–∞–≤–∏–º" (melt) DataFrame
        # –ö–æ–ª–æ–Ω–∫–∏ station_b - —ç—Ç–æ –≤—Å–µ, –ö–†–û–ú–ï 'num_pp' –∏ 'station_a'
        col_station_b_numeric = [col for col in df.columns if col not in ['num_pp', 'station_a']]
        
        df_long = df.melt(
            id_vars=['station_a'], 
            value_vars=col_station_b_numeric, 
            var_name='station_b_num', 
            value_name='distance'
        )
        
        # 7. –û—á–∏—Å—Ç–∫–∞
        df_long['station_a'] = df_long['station_a'].astype(str).str.strip()
        df_long['station_b_num'] = df_long['station_b_num'].astype(str).str.strip()
        
        # 8. –û—á–∏—â–∞–µ–º –æ—Ç –Ω–µ—á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ int
        df_long = df_long[pd.to_numeric(df_long['distance'], errors='coerce').notna()]
        df_long['distance'] = df_long['distance'].astype(int)
        
        # 9. –£–¥–∞–ª—è–µ–º –º–∞—Ä—à—Ä—É—Ç—ã —Å 0 –∫–º
        df_long = df_long[df_long['distance'] > 0]
        
        # 10. *** –ì–õ–ê–í–ù–´–ô –§–ò–ö–°: –ó–∞–º–µ–Ω—è–µ–º '1', '2' –Ω–∞ –∏–º–µ–Ω–∞ ***
        df_long['station_b'] = df_long['station_b_num'].map(header_map)
        
        # 11. –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –∑–∞–º–µ–Ω–∏–ª–æ—Å—å
        if df_long['station_b'].isnull().any():
            missing_keys = df_long[df_long['station_b'].isnull()]['station_b_num'].unique()
            log.warning(f"‚ö†Ô∏è –í {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∏–º–µ–Ω–∞ –¥–ª—è station_b –∫–ª—é—á–µ–π: {missing_keys[:10]}...")
            df_long.dropna(subset=['station_b'], inplace=True)

        # 12. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –Ω–µ–Ω—É–∂–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü
        df_long = df_long[['station_a', 'station_b', 'distance']]
        df_long.drop_duplicates(subset=['station_a', 'station_b'], keep='first', inplace=True)
        
        log.info(f"‚úÖ –ú–∞—Ç—Ä–∏—Ü–∞ {os.path.basename(filepath)} –∑–∞–≥—Ä—É–∂–µ–Ω–∞, {len(df_long)} –£–ù–ò–ö–ê–õ–¨–ù–´–• –º–∞—Ä—à—Ä—É—Ç–æ–≤.")
        return df_long
        
    except FileNotFoundError:
        log.error(f"‚ùå –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª '{filepath}'.")
        return None
    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –º–∞—Ç—Ä–∏—Ü—ã {filepath}: {e}", exc_info=True)
        return None
# --- üêû –ö–û–ù–ï–¶ –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ô –§–£–ù–ö–¶–ò–ò üêû ---


# --- 4. –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –º–∏–≥—Ä–∞—Ü–∏–∏ ---

async def main_migrate():
    '''
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è. –ü–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è, —Å–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã, –∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ.
    '''
    if not TARIFF_DB_URL:
        log.error("‚ùå TARIFF_DATABASE_URL –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ .env —Ñ–∞–π–ª–µ. –ú–∏–≥—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞.")
        return
        
    log.info(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –Ω–æ–≤–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —Ç–∞—Ä–∏—Ñ–æ–≤: {TARIFF_DB_URL.split('@')[-1]}")
    
    # 1. –°–æ–∑–¥–∞–µ–º –¥–≤–∏–∂–æ–∫ –∏ —Ç–∞–±–ª–∏—Ü—ã
    engine = create_async_engine(TARIFF_DB_URL)
    async with engine.begin() as conn:
        log.info("–û—á–∏—Å—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–∞–±–ª–∏—Ü (–µ—Å–ª–∏ –µ—Å—Ç—å)...")
        await conn.run_sync(Base.metadata.drop_all)
        log.info("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ç–∞–±–ª–∏—Ü (tariff_stations, tariff_matrix)...")
        await conn.run_sync(Base.metadata.create_all)
    
    Session = async_sessionmaker(engine, expire_on_commit=False)
    
    # 2. –ú–∏–≥—Ä–∞—Ü–∏—è —Å—Ç–∞–Ω—Ü–∏–π (2-–†–ü.csv)
    log.info("--- 1/2: –ù–∞—á–∏–Ω–∞—é –º–∏–≥—Ä–∞—Ü–∏—é –°—Ç–∞–Ω—Ü–∏–π (2-–†–ü.csv) ---")
    
    # üêû –ü–†–ï–î–ü–û–õ–û–ñ–ï–ù–ò–ï: –§–∞–π–ª—ã –ª–µ–∂–∞—Ç –≤ 'zdtarif_bot/data'
    # –ï—Å–ª–∏ —Å–∫—Ä–∏–ø—Ç –ª–µ–∂–∏—Ç –≤ –¥—Ä—É–≥–æ–º –º–µ—Å—Ç–µ, –∏–∑–º–µ–Ω–∏—Ç–µ —ç—Ç–æ—Ç –ø—É—Ç—å
    data_dir_path = os.path.join(project_root_dir, 'zdtarif_bot', 'data')
    
    # –ï—Å–ª–∏ –ø–∞–ø–∫–∏ 'zdtarif_bot/data' –Ω–µ—Ç, –∏—â–µ–º 'data'
    if not os.path.exists(data_dir_path):
        data_dir_path = os.path.join(project_root_dir, 'data')
        if not os.path.exists(data_dir_path):
             log.error(f"‚ùå –ù–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –ø–∞–ø–∫—É 'data' –∏–ª–∏ 'zdtarif_bot/data' –≤ {project_root_dir}")
             return
    
    log.info(f"–ò—Å–ø–æ–ª—å–∑—É—é –ø–∞–ø–∫—É —Å –¥–∞–Ω–Ω—ã–º–∏: {data_dir_path}")
    
    stations_df = load_kniga_2_rp(os.path.join(data_dir_path, '2-–†–ü.csv'))
    
    if stations_df is not None:
        
        stations_df = stations_df.where(pd.notnull(stations_df), None)

        async with Session() as session:
            async with session.begin():
                stations_to_add = []
                for _, row in stations_df.iterrows():
                    stations_to_add.append(
                        TariffStation(
                            name=row['station_name'],
                            code=row['station_code'],
                            railway=row['railway'],
                            operations=row['operations'],
                            transit_points=parse_transit_points_for_db(row['transit_points_raw'])
                        )
                    )
                log.info(f"–î–æ–±–∞–≤–ª—è—é {len(stations_to_add)} —Å—Ç–∞–Ω—Ü–∏–π –≤ –±–∞–∑—É...")
                session.add_all(stations_to_add)
            await session.commit()
        log.info("‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è —Å—Ç–∞–Ω—Ü–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
    else:
        log.error("‚ùå –ú–∏–≥—Ä–∞—Ü–∏—è —Å—Ç–∞–Ω—Ü–∏–π –ø—Ä–æ–≤–∞–ª–µ–Ω–∞, —Ñ–∞–π–ª 2-–†–ü.csv –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω.")
        return

    # 3. –ú–∏–≥—Ä–∞—Ü–∏—è (–¢–û–õ–¨–ö–û 3-1 –∏ 3-2 –†–æ—Å)
    log.info("--- 2/2: –ù–∞—á–∏–Ω–∞—é –º–∏–≥—Ä–∞—Ü–∏—é –ú–∞—Ç—Ä–∏—Ü (3-1 –†–æ—Å, 3-2 –†–æ—Å) ---")
    
    matrix_files = [
        os.path.join(data_dir_path, '3-1 –†–æ—Å.csv'),
        os.path.join(data_dir_path, '3-2 –†–æ—Å.csv')
    ]

    total_routes_added = 0
    
    async with Session() as session:
        for filepath in matrix_files:
            if not os.path.exists(filepath):
                log.error(f"‚ùå –§–∞–π–ª –º–∞—Ç—Ä–∏—Ü—ã {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–ø—É—Å–∫.")
                continue
                
            log.info(f"--- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {os.path.basename(filepath)} ---")
            matrix_df = load_kniga_3_matrix(filepath)
            
            if matrix_df is not None and not matrix_df.empty:
                async with session.begin():
                    log.info(f"–î–æ–±–∞–≤–ª—è—é {len(matrix_df)} –º–∞—Ä—à—Ä—É—Ç–æ–≤ (—Å –ø—Ä–æ–ø—É—Å–∫–æ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)...")
                    try:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º "upsert" (ON CONFLICT DO NOTHING)
                        for record in matrix_df.to_dict(orient='records'):
                            stmt = pg_insert(TariffMatrix).values(**record).on_conflict_do_nothing(
                                index_elements=['station_a', 'station_b']
                            )
                            await session.execute(stmt)
                        
                        total_routes_added += len(matrix_df) # –°—á–∏—Ç–∞–µ–º, —Å–∫–æ–ª—å–∫–æ –ü–û–ü–´–¢–ê–õ–ò–°–¨ –¥–æ–±–∞–≤–∏—Ç—å
                        
                    except Exception as e:
                        log.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤–∫–µ {os.path.basename(filepath)}: {e}", exc_info=True)
                        await session.rollback()
                await session.commit()
            else:
                log.warning(f"–§–∞–π–ª {os.path.basename(filepath)} –ø—Ä–æ–ø—É—â–µ–Ω (–ø—É—Å—Ç–æ–π –∏–ª–∏ –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏).")

    log.info(f"‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ü–æ–ø—ã—Ç–æ–∫ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è: {total_routes_added}")

    log.info("üéâüéâüéâ == –ú–ò–ì–†–ê–¶–ò–Ø –¢–ê–†–ò–§–ù–û–ô –ë–ê–ó–´ –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–ê! ==")
    
    await engine.dispose()


if __name__ == "__main__":
    # üêû –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –ø—É—Ç–∏ –∫ .env
    env_path = os.path.join(project_root_dir, '.env')
    if os.path.exists(env_path):
        log.info(f"–ó–∞–≥—Ä—É–∂–∞—é .env –∏–∑ {env_path}")
        load_dotenv(dotenv_path=env_path)
    else:
        log.warning(f"–§–∞–π–ª .env –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {project_root_dir}, –∏—Å–ø–æ–ª—å–∑—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã.")
        
    TARIFF_DB_URL = os.getenv("TARIFF_DATABASE_URL")
    
    asyncio.run(main_migrate())