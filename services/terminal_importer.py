# services/terminal_importer.py
from __future__ import annotations

import logging
import os
import re
import asyncio
import datetime
from datetime import timedelta
from zoneinfo import ZoneInfo
from typing import Optional, Dict, Any, Tuple, List

import pandas as pd
from sqlalchemy import text, update
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.ext.asyncio import AsyncSession

# --- Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ñ‹ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð° ---
from db import SessionLocal
from services.imap_service import ImapService
from model.terminal_container import TerminalContainer
from imap_tools.query import AND

logger = logging.getLogger(__name__)

# --- ÐšÐžÐÐ¡Ð¢ÐÐÐ¢Ð« Ð˜ ÐÐÐ¡Ð¢Ð ÐžÐ™ÐšÐ˜ ---
DOWNLOAD_DIR = "download_container"
os.makedirs(DOWNLOAD_DIR, exist_ok=True)

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¿Ð¾Ð¸ÑÐºÐ°
SUBJECT_FILTER_TERMINAL = r'executive\s*summary|A-Terminal'
SENDER_FILTER_TERMINAL = 'aterminal@effex.ru'
FILENAME_PATTERN_TERMINAL = r'\.(xlsx|xls|csv)$'

CLIENT_COLUMN_INDEX = 11 

# --- Ð’Ð¡ÐŸÐžÐœÐžÐ“ÐÐ¢Ð•Ð›Ð¬ÐÐ«Ð• Ð¤Ð£ÐÐšÐ¦Ð˜Ð˜ ---

def _get_vladivostok_date_str(days_offset: int = 0) -> str:
    """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð´Ð°Ñ‚Ñƒ Ð²Ð¾ Ð’Ð»Ð°Ð´Ð¸Ð²Ð¾ÑÑ‚Ð¾ÐºÐµ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ Ð”Ð”.ÐœÐœ.Ð“Ð“Ð“Ð“."""
    try:
        tz = ZoneInfo("Asia/Vladivostok")
    except Exception:
        tz = datetime.timezone(datetime.timedelta(hours=10))
    target_date = datetime.datetime.now(tz) - timedelta(days=days_offset)
    return target_date.strftime("%d.%m.%Y")

def clean_string_value(val: Any) -> Optional[str]:
    """ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÑ‚ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð² ÑÑ‚Ñ€Ð¾ÐºÑƒ, ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ñ Ñ‡Ð¸ÑÐ»Ð°."""
    if pd.isna(val) or val == '' or str(val).lower() == 'nan':
        return None
    try:
        if isinstance(val, float) and val.is_integer():
            return str(int(val))
        if isinstance(val, (int, float)):
            return str(int(val))
    except Exception:
        pass
    return str(val).strip()

def normalize_container(value: Any) -> Optional[str]:
    """ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÑ‚ Ð½Ð¾Ð¼ÐµÑ€ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð°."""
    s = clean_string_value(value)
    if not s:
        return None
    s = s.upper()
    s = re.sub(r'[^A-Z0-9]', '', s)
    if len(s) == 11:
        return s
    return s if s else None

def parse_date_safe(val: Any) -> Optional[datetime.date]:
    """Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ð¹ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Ð´Ð°Ñ‚Ñ‹."""
    if pd.isna(val) or val == '': return None
    try:
        if isinstance(val, (pd.Timestamp, datetime.datetime)): return val.date()
        return pd.to_datetime(val, dayfirst=True).date()
    except Exception: return None

def parse_time_safe(val: Any) -> Optional[datetime.time]:
    """Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ð¹ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸."""
    if pd.isna(val) or val == '': return None
    try:
        if isinstance(val, (pd.Timestamp, datetime.datetime)): return val.time()
        if isinstance(val, datetime.time): return val
        return pd.to_datetime(val, dayfirst=True).time()
    except Exception: return None

def parse_float_safe(val: Any) -> Optional[float]:
    """Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ð¹ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Ñ‡Ð¸ÑÐ»Ð°."""
    if pd.isna(val) or val == '': return None
    try:
        clean_val = str(val).replace(',', '.').replace('\xa0', '').replace(' ', '').strip()
        return float(clean_val)
    except Exception: return None

def extract_train_code_from_filename(filename: str) -> str | None:
    if not filename: return None
    base = os.path.basename(filename)
    name, _ = os.path.splitext(base)
    m = re.search(r"([ÐšK]\s*\d{2}[-â€“â€” ]?\s*\d{3})", name, flags=re.IGNORECASE)
    if not m: return None
    code = m.group(1).upper().replace("K", "Ðš").replace(" ", "").replace("â€“", "-").replace("â€”", "-")
    return code

def find_container_column(df: pd.DataFrame) -> str | None:
    candidates = ["ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€", "container", "container no", "Ð½Ð¾Ð¼ÐµÑ€ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð°", "â„– ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð°"]
    cols_norm = {str(c).strip().lower(): str(c) for c in df.columns}
    for cand in candidates:
        if cand in cols_norm: return cols_norm[cand]
    return None

# =========================================================================
# 1. Ð›ÐžÐ“Ð˜ÐšÐ Ð”Ð›Ð¯ ÐŸÐ›ÐÐÐ˜Ð ÐžÐ’Ð©Ð˜ÐšÐ
# =========================================================================

async def check_and_process_terminal_report() -> Optional[Dict[str, Any]]:
    imap = ImapService()
    filepath = None
    
    # Ð˜Ñ‰ÐµÐ¼ Ð·Ð° ÑÐµÐ³Ð¾Ð´Ð½Ñ Ð¸ Ð²Ñ‡ÐµÑ€Ð°
    for offset in [0, 1]:
        date_str = _get_vladivostok_date_str(days_offset=offset)
        logger.info(f"[Terminal Check] Ð˜Ñ‰Ñƒ Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð·Ð° {date_str}...")
        subject_regex = fr"({SUBJECT_FILTER_TERMINAL}).*{re.escape(date_str)}"
        
        filepath = await asyncio.to_thread(
            imap.download_latest_attachment,
            subject_filter=subject_regex,
            sender_filter=SENDER_FILTER_TERMINAL,
            filename_pattern=FILENAME_PATTERN_TERMINAL
        )
        if filepath: break

    if not filepath:
        logger.info("[Terminal Check] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½.")
        return None

    stats = None
    try:
        logger.info(f"[Terminal Check] ÐÐ°Ð¹Ð´ÐµÐ½: {filepath}. Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚...")
        async with SessionLocal() as session:
            import_result = await process_terminal_report_file(session, filepath)
            stats = {
                "file_name": os.path.basename(filepath),
                "status": "success",
                "total_added": import_result.get('added', 0),
                "total_updated": import_result.get('updated', 0)
            }
    except Exception as e:
        logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð°: {e}", exc_info=True)
        stats = {"error": str(e)}
    finally:
        if filepath and os.path.exists(filepath):
            os.remove(filepath)

    return stats

# =========================================================================
# 2. ÐžÐ¡ÐÐžÐ’ÐÐžÐ™ ÐŸÐ ÐžÐ¦Ð•Ð¡Ð¡ÐžÐ  Ð¤ÐÐ™Ð›ÐžÐ’
# =========================================================================

async def process_terminal_report_file(session: AsyncSession, file_path: str) -> dict:
    """
    ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚ Ñ‚Ð¸Ð¿ Ñ„Ð°Ð¹Ð»Ð° (Excel Ð¸Ð»Ð¸ CSV) Ð¸ Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰ÑƒÑŽ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ.
    """
    ext = os.path.splitext(file_path)[1].lower()
    
    if ext == '.csv':
        return await _process_csv_flat_file(session, file_path)
    else:
        return await _process_excel_split_file(session, file_path)

def _parse_row_data(row: pd.Series) -> Optional[dict]:
    """
    Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° ÑÑ‚Ñ€Ð¾ÐºÐ¸ (Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð¸ Ð´Ð»Ñ CSV, Ð¸ Ð´Ð»Ñ Excel).
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ Ð´Ð»Ñ Ð²ÑÑ‚Ð°Ð²ÐºÐ¸ Ð² Ð‘Ð” Ð¸Ð»Ð¸ None, ÐµÑÐ»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð½ÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾.
    """
    # 1. ÐÐ¾Ð¼ÐµÑ€ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð°
    cont_val = row.get('ÐšÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€')
    container_number = normalize_container(cont_val)
    if not container_number:
        return None

    # 2. Ð”Ð°Ñ‚Ñ‹
    accept_val = row.get('ÐŸÑ€Ð¸Ð½ÑÑ‚')
    dispatch_val = row.get('ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½')
    
    accept_date = parse_date_safe(accept_val)
    accept_time = parse_time_safe(accept_val)
    dispatch_date = parse_date_safe(dispatch_val)
    dispatch_time = parse_time_safe(dispatch_val)
    
    # 3. Ð¡Ñ‚Ð°Ñ‚ÑƒÑ
    status = 'ARRIVED'
    if dispatch_date:
        status = 'DISPATCHED'

    # 4. Ð’ÐµÑÐ° (Ñ Ñ€Ð°ÑÑ‡ÐµÑ‚Ð¾Ð¼ ÐÐµÑ‚Ñ‚Ð¾)
    weight_client = parse_float_safe(row.get('Ð‘Ñ€ÑƒÑ‚Ñ‚Ð¾ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°'))
    weight_terminal = parse_float_safe(row.get('Ð‘Ñ€ÑƒÑ‚Ñ‚Ð¾ Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ð°Ð»Ð°'))
    tare = parse_float_safe(row.get('Ð¢Ð°Ñ€Ð°'))
    
    weight_netto = None
    if weight_client is not None and tare is not None and weight_client > tare:
        weight_netto = weight_client - tare

    # 5. Ð¡Ð±Ð¾Ñ€ÐºÐ° Ð¾Ð±ÑŠÐµÐºÑ‚Ð°
    # ÐŸÑ€Ð¸Ð¼ÐµÑ‡Ð°Ð½Ð¸Ðµ: Ð’ Excel/CSV Pandas Ð¼Ð¾Ð¶ÐµÑ‚ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÑ‚ÑŒ ÑÑƒÑ„Ñ„Ð¸ÐºÑÑ‹ .1 Ð´Ð»Ñ Ð´ÑƒÐ±Ð»Ð¸Ñ€ÑƒÑŽÑ‰Ð¸Ñ…ÑÑ Ð¸Ð¼ÐµÐ½ ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº (Id, Ð¢Ñ€Ð°Ð½ÑÐ¿Ð¾Ñ€Ñ‚ Ð¸ Ñ‚.Ð´.)
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ ÐºÐ»ÑŽÑ‡ÐµÐ¹ Ñ ÑÑƒÑ„Ñ„Ð¸ÐºÑÐ°Ð¼Ð¸
    
    def get_val(key, key_suffix=''):
        """Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾Ðµ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð¸Ð· row Ñ ÑƒÑ‡ÐµÑ‚Ð¾Ð¼ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ñ‹Ñ… ÑÑƒÑ„Ñ„Ð¸ÐºÑÐ¾Ð² pandas"""
        val = row.get(f"{key}{key_suffix}")
        return clean_string_value(val)

    data = {
        'container_number': container_number,
        'terminal': get_val('Ð¢ÐµÑ€Ð¼Ð¸Ð½Ð°Ð»', ''), # ÐžÐ±Ñ‹Ñ‡Ð½Ð¾ Ð´ÐµÑ„Ð¾Ð»Ñ‚ A-Terminal
        'zone': get_val('Ð—Ð¾Ð½Ð°'),
        'inn': get_val('Ð˜ÐÐ'),
        'short_name': get_val('ÐšÑ€Ð°Ñ‚ÐºÐ¾Ðµ Ð½Ð°Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸Ðµ'),
        'client': get_val('ÐšÐ»Ð¸ÐµÐ½Ñ‚'),
        'stock': get_val('Ð¡Ñ‚Ð¾Ðº'),
        'customs_mode': get_val('Ð¢Ð°Ð¼Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼'),
        'direction': get_val('ÐÐ°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ'),
        'container_type': get_val('Ð¢Ð¸Ð¿'),
        'size': get_val('Ð Ð°Ð·Ð¼ÐµÑ€'),
        'payload': parse_float_safe(row.get('Ð“Ñ€ÑƒÐ·Ð¾Ð¿Ð¾Ð´ÑŠÑ‘Ð¼Ð½Ð¾ÑÑ‚ÑŒ')),
        
        'tare': tare,
        'manufacture_year': get_val('Ð“Ð¾Ð´ Ð¸Ð·Ð³Ð¾Ñ‚Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ'),
        'weight_client': weight_client,
        'weight_terminal': weight_terminal,
        'weight_netto': weight_netto,
        
        'state': get_val('Ð¡Ð¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ'),
        'cargo': get_val('Ð“Ñ€ÑƒÐ·'),
        'temperature': get_val('Ð¢ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð°'),
        'seals': get_val('ÐŸÐ»Ð¾Ð¼Ð±Ñ‹'),
        
        'accept_date': accept_date,
        'accept_time': accept_time,
        
        # Ð’Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ (Ð¿ÐµÑ€Ð²Ñ‹Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸)
        'in_id': get_val('Id'),
        'in_transport': get_val('Ð¢Ñ€Ð°Ð½ÑÐ¿Ð¾Ñ€Ñ‚'),
        'in_number': get_val('ÐÐ¾Ð¼ÐµÑ€ Ð²Ð°Ð³Ð¾Ð½Ð° | ÐÐ¾Ð¼ÐµÑ€ Ñ‚ÑÐ³Ð°Ñ‡Ð°'),
        'in_driver': get_val('Ð¡Ñ‚Ð°Ð½Ñ†Ð¸Ñ | Ð’Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒ'),
        
        'order_number': get_val('ÐÐ¾Ð¼ÐµÑ€ Ð·Ð°ÐºÐ°Ð·Ð°'),
        
        'dispatch_date': dispatch_date,
        'dispatch_time': dispatch_time,
        
        # Ð’Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ (Ð²Ñ‚Ð¾Ñ€Ñ‹Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸, Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾ Ñ ÑÑƒÑ„Ñ„Ð¸ÐºÑÐ¾Ð¼ .1 Ð² pandas, ÐµÑÐ»Ð¸ Ð¸Ð¼ÐµÐ½Ð° ÑÐ¾Ð²Ð¿Ð°Ð´Ð°ÑŽÑ‚)
        'out_id': get_val('Id', '.1') or get_val('Id.1'), 
        'out_transport': get_val('Ð¢Ñ€Ð°Ð½ÑÐ¿Ð¾Ñ€Ñ‚', '.1') or get_val('Ð¢Ñ€Ð°Ð½ÑÐ¿Ð¾Ñ€Ñ‚.1'),
        'out_number': get_val('ÐÐ¾Ð¼ÐµÑ€ Ð²Ð°Ð³Ð¾Ð½Ð° | ÐÐ¾Ð¼ÐµÑ€ Ñ‚ÑÐ³Ð°Ñ‡Ð°', '.1') or get_val('ÐÐ¾Ð¼ÐµÑ€ Ð²Ð°Ð³Ð¾Ð½Ð° | ÐÐ¾Ð¼ÐµÑ€ Ñ‚ÑÐ³Ð°Ñ‡Ð°.1'),
        'out_driver': get_val('Ð¡Ñ‚Ð°Ð½Ñ†Ð¸Ñ | Ð’Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒ', '.1') or get_val('Ð¡Ñ‚Ð°Ð½Ñ†Ð¸Ñ | Ð’Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒ.1'),
        
        'release': get_val('Ð ÐµÐ»Ð¸Ð·'),
        'carrier': get_val('ÐŸÐµÑ€ÐµÐ²Ð¾Ð·Ñ‡Ð¸Ðº'),
        'manager': get_val('ÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€'),
        'comment': get_val('ÐŸÑ€Ð¸Ð¼ÐµÑ‡Ð°Ð½Ð¸Ðµ'),
        
        'status': status
    }
    
    return data

# --- Ð’ÐÐ Ð˜ÐÐÐ¢ 1: CSV ---

async def _process_csv_flat_file(session: AsyncSession, file_path: str) -> dict:
    logger.info(f"[CSV Import] Ð§Ð¸Ñ‚Ð°ÑŽ Ñ„Ð°Ð¹Ð»: {file_path}")
    try:
        # sep=';' Ð´Ð»Ñ Ñ€ÑƒÑÑÐºÐ¾Ð³Ð¾ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð° CSV
        df = pd.read_csv(file_path, sep=';', dtype=str, on_bad_lines='skip')
        df.columns = df.columns.str.strip()
        
        processed_rows = []
        for _, row in df.iterrows():
            data = _parse_row_data(row)
            if data:
                processed_rows.append(data)
            
        if processed_rows:
            await _bulk_upsert_full_data(session, processed_rows)
            return {"added": len(processed_rows), "updated": 0}
            
        return {"added": 0, "updated": 0}
    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° CSV Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°: {e}", exc_info=True)
        raise e

# --- Ð’ÐÐ Ð˜ÐÐÐ¢ 2: EXCEL (Arrival + Dispatch) ---

async def _process_excel_split_file(session: AsyncSession, file_path: str) -> dict:
    logger.info(f"[Excel Import] Ð§Ð¸Ñ‚Ð°ÑŽ Excel: {file_path}")
    
    try:
        xls = pd.ExcelFile(file_path)
        all_rows = []
        
        # Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð»Ð¸ÑÑ‚Ð¾Ð² Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸. ÐŸÐ¾Ñ€ÑÐ´Ð¾Ðº Ð²Ð°Ð¶ÐµÐ½: Dispatch Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹, 
        # Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ð± Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐµ Ð¿ÐµÑ€ÐµÐ·Ð°Ð¿Ð¸ÑÐ°Ð»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¾ Ð¿Ñ€Ð¸Ð±Ñ‹Ñ‚Ð¸Ð¸, ÐµÑÐ»Ð¸ Ð¾Ð½Ð¸ Ð´ÑƒÐ±Ð»Ð¸Ñ€ÑƒÑŽÑ‚ÑÑ (Ð½Ð¾ upsert Ð¼ÐµÑ€Ð¶Ð¸Ñ‚ Ð¿Ð¾Ð»Ñ).
        # Ð’ Ð´Ð°Ð½Ð½Ð¾Ð¼ ÑÐ»ÑƒÑ‡Ð°Ðµ upsert Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚ Ð²ÑÐµ Ð¿Ð¾Ð»Ñ.
        sheets_to_process = []
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ Ð»Ð¸ÑÑ‚Ð¾Ð² (Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð¾Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ Ð¸Ð»Ð¸ Ð¿Ð¾ Ñ‡Ð°ÑÑ‚Ð¸Ñ‡Ð½Ð¾Ð¼Ñƒ ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸ÑŽ)
        for sheet_name in xls.sheet_names:
            lower_name = sheet_name.lower()
            if "arrival" in lower_name or "dispatch" in lower_name:
                sheets_to_process.append(sheet_name)
        
        if not sheets_to_process:
            logger.warning(f"Ð’ Ñ„Ð°Ð¹Ð»Ðµ {file_path} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð»Ð¸ÑÑ‚Ð¾Ð² Arrival Ð¸Ð»Ð¸ Dispatch.")
            return {"added": 0, "updated": 0}

        for sheet in sheets_to_process:
            logger.info(f"ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð»Ð¸ÑÑ‚Ð°: {sheet}")
            # Ð§Ð¸Ñ‚Ð°ÐµÐ¼ Ð»Ð¸ÑÑ‚
            df = pd.read_excel(xls, sheet_name=sheet, dtype=str)
            df.columns = df.columns.str.strip()
            
            for _, row in df.iterrows():
                data = _parse_row_data(row)
                if data:
                    all_rows.append(data)
        
        if all_rows:
            # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð¿Ð¾Ð»Ð½Ñ‹Ðµ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹ (ÐµÑÐ»Ð¸ Ð¾Ð´Ð¸Ð½ Ð¸ Ñ‚Ð¾Ñ‚ Ð¶Ðµ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€ Ð² Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ð»Ð¸ÑÑ‚Ð°Ñ… Ñ Ð¾Ð´Ð¸Ð½Ð°ÐºÐ¾Ð²Ñ‹Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸)
            # ÐÐ¾ Ð´Ð»Ñ Upsert ÑÑ‚Ð¾ Ð½Ðµ ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡Ð½Ð¾, Ð‘Ð” ÑÐ°Ð¼Ð° Ñ€Ð°Ð·Ð±ÐµÑ€ÐµÑ‚ÑÑ.
            # Ð’Ð°Ð¶Ð½ÐµÐµ, ÐµÑÐ»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ñ€Ð°Ð·Ð½Ñ‹Ðµ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ð² Dispatch Ð¿Ð¾ÑÐ²Ð¸Ð»Ð°ÑÑŒ Ð´Ð°Ñ‚Ð° Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐ¸).
            # Upsert Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚ Ð·Ð°Ð¿Ð¸ÑÑŒ.
            
            logger.info(f"ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð»ÐµÐ½Ð¾ {len(all_rows)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð´Ð»Ñ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð² Ð‘Ð”.")
            await _bulk_upsert_full_data(session, all_rows)
            return {"added": len(all_rows), "updated": 0}
            
        return {"added": 0, "updated": 0}

    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Excel Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°: {e}", exc_info=True)
        raise e


# =========================================================================
# 3. SQL Ð—ÐÐŸÐ ÐžÐ¡Ð«
# =========================================================================

async def _bulk_upsert_full_data(session: AsyncSession, rows: List[dict]):
    """
    Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ UPSERT Ð²ÑÐµÑ… Ð¿Ð¾Ð»ÐµÐ¹.
    """
    if not rows: return

    stmt = text("""
        INSERT INTO terminal_containers (
            container_number, terminal, zone, inn, short_name, client, stock,
            customs_mode, direction, container_type, size, payload, tare,
            manufacture_year, weight_client, weight_terminal, state, cargo,
            temperature, seals, accept_date, accept_time,
            in_id, in_transport, in_number, in_driver, order_number,
            dispatch_date, dispatch_time,
            out_id, out_transport, out_number, out_driver,
            release, carrier, manager, comment, status, weight_netto,
            created_at, updated_at
        ) VALUES (
            :container_number, :terminal, :zone, :inn, :short_name, :client, :stock,
            :customs_mode, :direction, :container_type, :size, :payload, :tare,
            :manufacture_year, :weight_client, :weight_terminal, :state, :cargo,
            :temperature, :seals, :accept_date, :accept_time,
            :in_id, :in_transport, :in_number, :in_driver, :order_number,
            :dispatch_date, :dispatch_time,
            :out_id, :out_transport, :out_number, :out_driver,
            :release, :carrier, :manager, :comment, :status, :weight_netto,
            NOW(), NOW()
        )
        ON CONFLICT (container_number) DO UPDATE SET
            terminal = EXCLUDED.terminal,
            zone = EXCLUDED.zone,
            inn = EXCLUDED.inn,
            short_name = EXCLUDED.short_name,
            client = EXCLUDED.client,
            stock = EXCLUDED.stock,
            customs_mode = EXCLUDED.customs_mode,
            direction = EXCLUDED.direction,
            container_type = EXCLUDED.container_type,
            size = EXCLUDED.size,
            payload = EXCLUDED.payload,
            tare = EXCLUDED.tare,
            manufacture_year = EXCLUDED.manufacture_year,
            weight_client = EXCLUDED.weight_client,
            weight_terminal = EXCLUDED.weight_terminal,
            state = EXCLUDED.state,
            cargo = EXCLUDED.cargo,
            temperature = EXCLUDED.temperature,
            seals = EXCLUDED.seals,
            accept_date = EXCLUDED.accept_date,
            accept_time = EXCLUDED.accept_time,
            in_id = EXCLUDED.in_id,
            in_transport = EXCLUDED.in_transport,
            in_number = EXCLUDED.in_number,
            in_driver = EXCLUDED.in_driver,
            order_number = EXCLUDED.order_number,
            dispatch_date = EXCLUDED.dispatch_date,
            dispatch_time = EXCLUDED.dispatch_time,
            out_id = EXCLUDED.out_id,
            out_transport = EXCLUDED.out_transport,
            out_number = EXCLUDED.out_number,
            out_driver = EXCLUDED.out_driver,
            release = EXCLUDED.release,
            carrier = EXCLUDED.carrier,
            manager = EXCLUDED.manager,
            comment = EXCLUDED.comment,
            status = EXCLUDED.status,
            weight_netto = EXCLUDED.weight_netto,
            updated_at = NOW();
    """)

    # Ð Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÐ¼ Ð½Ð° Ð¿Ð°Ñ‡ÐºÐ¸ Ð¿Ð¾ 500 Ð´Ð»Ñ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸
    batch_size = 500
    for i in range(0, len(rows), batch_size):
        batch = rows[i:i + batch_size]
        await session.execute(stmt, batch)
        await session.commit()

    logger.info(f"ðŸ’¾ [DB] ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Upsert Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½ Ð´Ð»Ñ {len(rows)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹.")

# --- ÐžÐ¡Ð¢ÐÐ›Ð¬ÐÐ«Ð• Ð¤Ð£ÐÐšÐ¦Ð˜Ð˜ (Ð±ÐµÐ· Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹) ---
async def _collect_containers_from_excel(file_path: str) -> Dict[str, str]:
    xl = pd.ExcelFile(file_path)
    container_client_map = {}
    for sheet in xl.sheet_names:
        try:
            df = pd.read_excel(xl, sheet_name=sheet)
            df.columns = [str(c).strip() for c in df.columns]
            col_container = find_container_column(df)
            col_client = df.columns[CLIENT_COLUMN_INDEX] if len(df.columns) > CLIENT_COLUMN_INDEX else None
            if not col_container: continue
            for _, row in df.iterrows():
                cn = normalize_container(row.get(col_container))
                cl_val = clean_string_value(row.get(col_client)) if col_client else None
                if cn: container_client_map[cn] = cl_val if cl_val else ""
        except Exception as e:
            logger.error(f"Error reading sheet {sheet}: {e}")
    return container_client_map

async def import_train_from_excel(src_file_path: str) -> Tuple[int, int, str]:
    train_code = extract_train_code_from_filename(src_file_path)
    if not train_code: raise ValueError("No train code")
    container_map = await _collect_containers_from_excel(src_file_path)
    if not container_map: return 0, 0, train_code
    updated_count = 0
    async with SessionLocal() as session:
        async with session.begin():
            for cn, client_name in container_map.items():
                stmt = update(TerminalContainer).where(TerminalContainer.container_number == cn).values(train=train_code, client=client_name)
                res = await session.execute(stmt)
                updated_count += res.rowcount
    return updated_count, len(container_map), train_code