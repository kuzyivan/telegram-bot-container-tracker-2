# services/terminal_importer.py
from __future__ import annotations

import logging
import os
import re
import asyncio
import datetime
from datetime import timedelta
from zoneinfo import ZoneInfo
from typing import Optional, Dict, Any, Tuple, List

import pandas as pd
from sqlalchemy import text, update
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.ext.asyncio import AsyncSession

# --- Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ñ‹ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð° ---
from db import SessionLocal
from services.imap_service import ImapService
from model.terminal_container import TerminalContainer
from imap_tools.query import AND

logger = logging.getLogger(__name__)

# --- ÐšÐžÐÐ¡Ð¢ÐÐÐ¢Ð« Ð˜ ÐÐÐ¡Ð¢Ð ÐžÐ™ÐšÐ˜ ---
DOWNLOAD_DIR = "download_container"
os.makedirs(DOWNLOAD_DIR, exist_ok=True)

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¿Ð¾Ð¸ÑÐºÐ° (Ð´Ð¾Ð±Ð°Ð²Ð¸Ð»Ð¸ csv Ð² Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½)
SUBJECT_FILTER_TERMINAL = r'executive\s*summary|A-Terminal'
SENDER_FILTER_TERMINAL = 'aterminal@effex.ru'
FILENAME_PATTERN_TERMINAL = r'\.(xlsx|xls|csv)$'

CLIENT_COLUMN_INDEX = 11 

# --- Ð’Ð¡ÐŸÐžÐœÐžÐ“ÐÐ¢Ð•Ð›Ð¬ÐÐ«Ð• Ð¤Ð£ÐÐšÐ¦Ð˜Ð˜ ---

def _get_vladivostok_date_str(days_offset: int = 0) -> str:
    """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð´Ð°Ñ‚Ñƒ Ð²Ð¾ Ð’Ð»Ð°Ð´Ð¸Ð²Ð¾ÑÑ‚Ð¾ÐºÐµ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ Ð”Ð”.ÐœÐœ.Ð“Ð“Ð“Ð“."""
    try:
        tz = ZoneInfo("Asia/Vladivostok")
    except Exception:
        tz = datetime.timezone(datetime.timedelta(hours=10))
    target_date = datetime.datetime.now(tz) - timedelta(days=days_offset)
    return target_date.strftime("%d.%m.%Y")

def clean_string_value(val: Any) -> Optional[str]:
    """ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÑ‚ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð² ÑÑ‚Ñ€Ð¾ÐºÑƒ, ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ñ Ñ‡Ð¸ÑÐ»Ð°."""
    if pd.isna(val) or val == '' or str(val).lower() == 'nan':
        return None
    try:
        # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ .0 Ñƒ Ñ†ÐµÐ»Ñ‹Ñ… Ñ‡Ð¸ÑÐµÐ», ÑÑ‚Ð°Ð²ÑˆÐ¸Ñ… float
        if isinstance(val, float) and val.is_integer():
            return str(int(val))
        if isinstance(val, (int, float)):
            return str(int(val))
    except Exception:
        pass
    return str(val).strip()

def normalize_container(value: Any) -> Optional[str]:
    """ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÑ‚ Ð½Ð¾Ð¼ÐµÑ€ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð°."""
    s = clean_string_value(value)
    if not s:
        return None
    s = s.upper()
    s = re.sub(r'[^A-Z0-9]', '', s)
    if len(s) == 11:
        return s
    return s if s else None

def parse_date_safe(val: Any) -> Optional[datetime.date]:
    """Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ð¹ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Ð´Ð°Ñ‚Ñ‹."""
    if pd.isna(val) or val == '': return None
    try:
        if isinstance(val, (pd.Timestamp, datetime.datetime)): return val.date()
        # ÐŸÑ€Ð¾Ð±ÑƒÐµÐ¼ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ñ‹ Ñ Ð²Ñ€ÐµÐ¼ÐµÐ½ÐµÐ¼ Ð¸ Ð±ÐµÐ·
        return pd.to_datetime(val, dayfirst=True).date()
    except Exception: return None

def parse_time_safe(val: Any) -> Optional[datetime.time]:
    """Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ð¹ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸."""
    if pd.isna(val) or val == '': return None
    try:
        if isinstance(val, (pd.Timestamp, datetime.datetime)): return val.time()
        if isinstance(val, datetime.time): return val
        return pd.to_datetime(val, dayfirst=True).time()
    except Exception: return None

def parse_float_safe(val: Any) -> Optional[float]:
    """Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ð¹ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Ñ‡Ð¸ÑÐ»Ð°."""
    if pd.isna(val) or val == '': return None
    try:
        # Ð—Ð°Ð¼ÐµÐ½ÑÐµÐ¼ Ð·Ð°Ð¿ÑÑ‚ÑƒÑŽ Ð½Ð° Ñ‚Ð¾Ñ‡ÐºÑƒ Ð¸ Ð½ÐµÑ€Ð°Ð·Ñ€Ñ‹Ð²Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹
        clean_val = str(val).replace(',', '.').replace('\xa0', '').replace(' ', '').strip()
        return float(clean_val)
    except Exception: return None

def extract_train_code_from_filename(filename: str) -> str | None:
    if not filename: return None
    base = os.path.basename(filename)
    name, _ = os.path.splitext(base)
    m = re.search(r"([ÐšK]\s*\d{2}[-â€“â€” ]?\s*\d{3})", name, flags=re.IGNORECASE)
    if not m: return None
    code = m.group(1).upper().replace("K", "Ðš").replace(" ", "").replace("â€“", "-").replace("â€”", "-")
    return code

def find_container_column(df: pd.DataFrame) -> str | None:
    candidates = ["ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€", "container", "container no", "Ð½Ð¾Ð¼ÐµÑ€ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð°", "â„– ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð°"]
    cols_norm = {str(c).strip().lower(): str(c) for c in df.columns}
    for cand in candidates:
        if cand in cols_norm: return cols_norm[cand]
    return None

# =========================================================================
# 1. Ð›ÐžÐ“Ð˜ÐšÐ Ð”Ð›Ð¯ ÐŸÐ›ÐÐÐ˜Ð ÐžÐ’Ð©Ð˜ÐšÐ
# =========================================================================

async def check_and_process_terminal_report() -> Optional[Dict[str, Any]]:
    imap = ImapService()
    filepath = None
    
    # Ð˜Ñ‰ÐµÐ¼ Ð·Ð° ÑÐµÐ³Ð¾Ð´Ð½Ñ Ð¸ Ð²Ñ‡ÐµÑ€Ð°
    for offset in [0, 1]:
        date_str = _get_vladivostok_date_str(days_offset=offset)
        logger.info(f"[Terminal Check] Ð˜Ñ‰Ñƒ Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð·Ð° {date_str}...")
        # Ð˜Ñ‰ÐµÐ¼ Ð»Ð¸Ð±Ð¾ Executive summary, Ð»Ð¸Ð±Ð¾ Ñ„Ð°Ð¹Ð»Ñ‹ A-Terminal
        subject_regex = fr"({SUBJECT_FILTER_TERMINAL}).*{re.escape(date_str)}"
        
        filepath = await asyncio.to_thread(
            imap.download_latest_attachment,
            subject_filter=subject_regex,
            sender_filter=SENDER_FILTER_TERMINAL,
            filename_pattern=FILENAME_PATTERN_TERMINAL
        )
        if filepath: break

    if not filepath:
        logger.info("[Terminal Check] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½.")
        return None

    stats = None
    try:
        logger.info(f"[Terminal Check] ÐÐ°Ð¹Ð´ÐµÐ½: {filepath}. Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚...")
        async with SessionLocal() as session:
            import_result = await process_terminal_report_file(session, filepath)
            stats = {
                "file_name": os.path.basename(filepath),
                "status": "success",
                "total_added": import_result.get('added', 0),
                "total_updated": import_result.get('updated', 0)
            }
    except Exception as e:
        logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð°: {e}", exc_info=True)
        stats = {"error": str(e)}
    finally:
        if filepath and os.path.exists(filepath):
            os.remove(filepath)

    return stats

# =========================================================================
# 2. ÐžÐ¡ÐÐžÐ’ÐÐžÐ™ ÐŸÐ ÐžÐ¦Ð•Ð¡Ð¡ÐžÐ  Ð¤ÐÐ™Ð›ÐžÐ’
# =========================================================================

async def process_terminal_report_file(session: AsyncSession, file_path: str) -> dict:
    """
    ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚ Ñ‚Ð¸Ð¿ Ñ„Ð°Ð¹Ð»Ð° (Excel Ð¸Ð»Ð¸ CSV) Ð¸ Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰ÑƒÑŽ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ.
    """
    ext = os.path.splitext(file_path)[1].lower()
    
    if ext == '.csv':
        return await _process_csv_flat_file(session, file_path)
    else:
        return await _process_excel_split_file(session, file_path)

# --- Ð’ÐÐ Ð˜ÐÐÐ¢ 1: CSV (ÐžÐ±Ñ‰Ð¸Ð¹ Ñ„Ð°Ð¹Ð», Ð’Ð¡Ð• ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸) ---

async def _process_csv_flat_file(session: AsyncSession, file_path: str) -> dict:
    """
    ÐŸÐ°Ñ€ÑÐ¸Ñ‚ CSV, Ð³Ð´Ðµ Ð²ÑÐµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² Ð¾Ð´Ð½Ð¾Ð¹ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ðµ.
    ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÑ‚ Ð°Ð±ÑÐ¾Ð»ÑŽÑ‚Ð½Ð¾ Ð²ÑÐµ Ð¿Ð¾Ð»Ñ.
    """
    logger.info(f"[CSV Import] Ð§Ð¸Ñ‚Ð°ÑŽ Ð¾Ð±Ñ‰Ð¸Ð¹ Ñ„Ð°Ð¹Ð»: {file_path}")
    
    try:
        # Ð§Ð¸Ñ‚Ð°ÐµÐ¼ CSV. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ sep=';' Ñ‚Ð°Ðº ÐºÐ°Ðº ÑÑ‚Ð¾ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚ Ð´Ð»Ñ 1C/Russian CSV
        # dtype=str Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð¿Ð¾Ñ‚ÐµÑ€ÑÑ‚ÑŒ Ð²ÐµÐ´ÑƒÑ‰Ð¸Ðµ Ð½ÑƒÐ»Ð¸
        df = pd.read_csv(file_path, sep=';', dtype=str)
        
        # ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð¸Ð¼ÐµÐ½ ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº Ð¾Ñ‚ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ð¾Ð²
        df.columns = df.columns.str.strip()
        
        processed_rows = []
        
        for _, row in df.iterrows():
            cont_val = row.get('ÐšÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€')
            container_number = normalize_container(cont_val)
            if not container_number:
                continue

            # --- ÐœÐ°Ð¿Ð¿Ð¸Ð½Ð³ Ð¿Ð¾Ð»ÐµÐ¹ ---
            # Ð£Ñ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼, Ñ‡Ñ‚Ð¾ pandas Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÑ‚ .1, .2 Ðº Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð°Ð¼ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¾Ð²
            
            # 1. Ð”Ð°Ñ‚Ñ‹
            accept_val = row.get('ÐŸÑ€Ð¸Ð½ÑÑ‚')
            dispatch_val = row.get('ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½')
            
            accept_date = parse_date_safe(accept_val)
            accept_time = parse_time_safe(accept_val)
            dispatch_date = parse_date_safe(dispatch_val)
            dispatch_time = parse_time_safe(dispatch_val)
            
            # 2. Ð¡Ñ‚Ð°Ñ‚ÑƒÑ
            status = 'ARRIVED'
            if dispatch_date:
                status = 'DISPATCHED'

            # --- ðŸ”¥ ÐÐ’Ð¢ÐžÐœÐÐ¢Ð˜Ð§Ð•Ð¡ÐšÐ˜Ð™ Ð ÐÐ¡Ð§Ð•Ð¢ Ð’Ð•Ð¡Ð ÐÐ•Ð¢Ð¢Ðž ---
            weight_client = parse_float_safe(row.get('Ð‘Ñ€ÑƒÑ‚Ñ‚Ð¾ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°'))
            weight_terminal = parse_float_safe(row.get('Ð‘Ñ€ÑƒÑ‚Ñ‚Ð¾ Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ð°Ð»Ð°'))
            tare = parse_float_safe(row.get('Ð¢Ð°Ñ€Ð°'))
            
            weight_netto = None
            if weight_client is not None and tare is not None and weight_client > tare:
                weight_netto = weight_client - tare
            # --------------------------------------------
                
            # 3. Ð¡Ð±Ð¾Ñ€ÐºÐ° Ð¾Ð±ÑŠÐµÐºÑ‚Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ…
            # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ .get() Ñ Ð´ÐµÑ„Ð¾Ð»Ñ‚Ð¾Ð¼, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð¿Ð°Ð´Ð°Ñ‚ÑŒ, ÐµÑÐ»Ð¸ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ Ð½ÐµÑ‚
            data = {
                'container_number': container_number,
                'terminal': clean_string_value(row.get('Ð¢ÐµÑ€Ð¼Ð¸Ð½Ð°Ð»', 'A-Terminal')),
                'zone': clean_string_value(row.get('Ð—Ð¾Ð½Ð°')),
                'inn': clean_string_value(row.get('Ð˜ÐÐ')),
                'short_name': clean_string_value(row.get('ÐšÑ€Ð°Ñ‚ÐºÐ¾Ðµ Ð½Ð°Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸Ðµ')),
                'client': clean_string_value(row.get('ÐšÐ»Ð¸ÐµÐ½Ñ‚')),
                'stock': clean_string_value(row.get('Ð¡Ñ‚Ð¾Ðº')),
                'customs_mode': clean_string_value(row.get('Ð¢Ð°Ð¼Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼')),
                'direction': clean_string_value(row.get('ÐÐ°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ')),
                'container_type': clean_string_value(row.get('Ð¢Ð¸Ð¿')),
                'size': clean_string_value(row.get('Ð Ð°Ð·Ð¼ÐµÑ€')),
                'payload': parse_float_safe(row.get('Ð“Ñ€ÑƒÐ·Ð¾Ð¿Ð¾Ð´ÑŠÑ‘Ð¼Ð½Ð¾ÑÑ‚ÑŒ')),
                
                'tare': tare, # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ
                'manufacture_year': clean_string_value(row.get('Ð“Ð¾Ð´ Ð¸Ð·Ð³Ð¾Ñ‚Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ')),
                'weight_client': weight_client, # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ
                'weight_terminal': weight_terminal, # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ
                
                'state': clean_string_value(row.get('Ð¡Ð¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ')),
                'cargo': clean_string_value(row.get('Ð“Ñ€ÑƒÐ·')),
                'temperature': clean_string_value(row.get('Ð¢ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð°')),
                'seals': clean_string_value(row.get('ÐŸÐ»Ð¾Ð¼Ð±Ñ‹')),
                
                'accept_date': accept_date,
                'accept_time': accept_time,
                
                # Ð’Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ (Ð¿ÐµÑ€Ð²Ñ‹Ðµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸)
                'in_id': clean_string_value(row.get('Id')),
                'in_transport': clean_string_value(row.get('Ð¢Ñ€Ð°Ð½ÑÐ¿Ð¾Ñ€Ñ‚')),
                'in_number': clean_string_value(row.get('ÐÐ¾Ð¼ÐµÑ€ Ð²Ð°Ð³Ð¾Ð½Ð° | ÐÐ¾Ð¼ÐµÑ€ Ñ‚ÑÐ³Ð°Ñ‡Ð°')),
                'in_driver': clean_string_value(row.get('Ð¡Ñ‚Ð°Ð½Ñ†Ð¸Ñ | Ð’Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒ')),
                
                'order_number': clean_string_value(row.get('ÐÐ¾Ð¼ÐµÑ€ Ð·Ð°ÐºÐ°Ð·Ð°')),
                
                'dispatch_date': dispatch_date,
                'dispatch_time': dispatch_time,
                
                # Ð’Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ (Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹ ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº Ð¸Ð¼ÐµÑŽÑ‚ ÑÑƒÑ„Ñ„Ð¸ÐºÑ .1 Ð² pandas)
                'out_id': clean_string_value(row.get('Id.1')),
                'out_transport': clean_string_value(row.get('Ð¢Ñ€Ð°Ð½ÑÐ¿Ð¾Ñ€Ñ‚.1')),
                'out_number': clean_string_value(row.get('ÐÐ¾Ð¼ÐµÑ€ Ð²Ð°Ð³Ð¾Ð½Ð° | ÐÐ¾Ð¼ÐµÑ€ Ñ‚ÑÐ³Ð°Ñ‡Ð°.1')),
                'out_driver': clean_string_value(row.get('Ð¡Ñ‚Ð°Ð½Ñ†Ð¸Ñ | Ð’Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒ.1')),
                
                'release': clean_string_value(row.get('Ð ÐµÐ»Ð¸Ð·')),
                'carrier': clean_string_value(row.get('ÐŸÐµÑ€ÐµÐ²Ð¾Ð·Ñ‡Ð¸Ðº')),
                'manager': clean_string_value(row.get('ÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€')),
                'comment': clean_string_value(row.get('ÐŸÑ€Ð¸Ð¼ÐµÑ‡Ð°Ð½Ð¸Ðµ')),
                
                'status': status,
                
                # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ñ€Ð°ÑÑÑ‡Ð¸Ñ‚Ð°Ð½Ð½Ð¾Ðµ Ð½ÐµÑ‚Ñ‚Ð¾
                'weight_netto': weight_netto
            }
            processed_rows.append(data)
            
        if processed_rows:
            # Ð’Ñ‹Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¼Ð¾Ñ‰Ð½Ñ‹Ð¹ UPSERT Ð´Ð»Ñ Ð²ÑÐµÑ… Ð¿Ð¾Ð»ÐµÐ¹
            await _bulk_upsert_full_data(session, processed_rows)
            return {"added": len(processed_rows), "updated": 0} # Ð”Ð»Ñ CSV ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼ Ð²ÑÐµ ÐºÐ°Ðº "Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ðµ"
            
        return {"added": 0, "updated": 0}

    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° CSV Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°: {e}", exc_info=True)
        raise e

# --- Ð’ÐÐ Ð˜ÐÐÐ¢ 2: EXCEL (Ð¡Ñ‚Ð°Ñ€Ð°Ñ Ð»Ð¾Ð³Ð¸ÐºÐ° Ñ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸ÐµÐ¼ Ð»Ð¸ÑÑ‚Ð¾Ð²) ---

async def _process_excel_split_file(session: AsyncSession, file_path: str) -> dict:
    """
    ÐŸÐ°Ñ€ÑÐ¸Ñ‚ Excel Ñ Ð»Ð¸ÑÑ‚Ð°Ð¼Ð¸ Arrival / Dispatch.
    """
    
    logger.info(f"[Excel Import] Ð§Ð¸Ñ‚Ð°ÑŽ Excel: {file_path}")
    xls = pd.ExcelFile(file_path)
    added = 0
    updated = 0
    
    # 1. Arrival
    if "Arrival" in xls.sheet_names:
        df = pd.read_excel(xls, sheet_name="Arrival", dtype=str)
        # ... (Ñ‚ÑƒÑ‚ Ð´Ð¾Ð»Ð¶Ð½Ð° Ð±Ñ‹Ñ‚ÑŒ Ð»Ð¾Ð³Ð¸ÐºÐ° Ð¼Ð°Ð¿Ð¿Ð¸Ð½Ð³Ð° Ð´Ð»Ñ ÑÑ‚Ð°Ñ€Ð¾Ð³Ð¾ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð° Excel, ÐµÑÐ»Ð¸ Ð¾Ð½ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ) ...
        # Ð”Ð»Ñ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ñ‚Ñ‹ Ð¸ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ñ CSV-Ð¾Ñ€Ð¸ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸ÐµÐ¼, 
        # Ð»ÑƒÑ‡ÑˆÐµ ÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Excel Ð² CSV Ð¸Ð»Ð¸ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¼Ð°Ð¿Ð¿Ð¸Ð½Ð³.
        pass 

    # 2. Dispatch
    if "Dispatch" in xls.sheet_names:
        df = pd.read_excel(xls, sheet_name="Dispatch", dtype=str)
        # ...
        pass
        
    return {"added": added, "updated": updated}


# =========================================================================
# 3. SQL Ð—ÐÐŸÐ ÐžÐ¡Ð«
# =========================================================================

async def _bulk_upsert_full_data(session: AsyncSession, rows: List[dict]):
    """
    Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ UPSERT Ð²ÑÐµÑ… Ð¿Ð¾Ð»ÐµÐ¹.
    Ð•ÑÐ»Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€ ÐµÑÑ‚ÑŒ -> Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð’Ð¡Ð.
    Ð•ÑÐ»Ð¸ Ð½ÐµÑ‚ -> Ð²ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼.
    """
    if not rows: return

    stmt = text("""
        INSERT INTO terminal_containers (
            container_number, terminal, zone, inn, short_name, client, stock,
            customs_mode, direction, container_type, size, payload, tare,
            manufacture_year, weight_client, weight_terminal, state, cargo,
            temperature, seals, accept_date, accept_time,
            in_id, in_transport, in_number, in_driver, order_number,
            dispatch_date, dispatch_time,
            out_id, out_transport, out_number, out_driver,
            release, carrier, manager, comment, status, weight_netto,
            created_at, updated_at
        ) VALUES (
            :container_number, :terminal, :zone, :inn, :short_name, :client, :stock,
            :customs_mode, :direction, :container_type, :size, :payload, :tare,
            :manufacture_year, :weight_client, :weight_terminal, :state, :cargo,
            :temperature, :seals, :accept_date, :accept_time,
            :in_id, :in_transport, :in_number, :in_driver, :order_number,
            :dispatch_date, :dispatch_time,
            :out_id, :out_transport, :out_number, :out_driver,
            :release, :carrier, :manager, :comment, :status, :weight_netto,
            NOW(), NOW()
        )
        ON CONFLICT (container_number) DO UPDATE SET
            terminal = EXCLUDED.terminal,
            zone = EXCLUDED.zone,
            inn = EXCLUDED.inn,
            short_name = EXCLUDED.short_name,
            client = EXCLUDED.client,
            stock = EXCLUDED.stock,
            customs_mode = EXCLUDED.customs_mode,
            direction = EXCLUDED.direction,
            container_type = EXCLUDED.container_type,
            size = EXCLUDED.size,
            payload = EXCLUDED.payload,
            tare = EXCLUDED.tare,
            manufacture_year = EXCLUDED.manufacture_year,
            weight_client = EXCLUDED.weight_client,
            weight_terminal = EXCLUDED.weight_terminal,
            state = EXCLUDED.state,
            cargo = EXCLUDED.cargo,
            temperature = EXCLUDED.temperature,
            seals = EXCLUDED.seals,
            accept_date = EXCLUDED.accept_date,
            accept_time = EXCLUDED.accept_time,
            in_id = EXCLUDED.in_id,
            in_transport = EXCLUDED.in_transport,
            in_number = EXCLUDED.in_number,
            in_driver = EXCLUDED.in_driver,
            order_number = EXCLUDED.order_number,
            dispatch_date = EXCLUDED.dispatch_date,
            dispatch_time = EXCLUDED.dispatch_time,
            out_id = EXCLUDED.out_id,
            out_transport = EXCLUDED.out_transport,
            out_number = EXCLUDED.out_number,
            out_driver = EXCLUDED.out_driver,
            release = EXCLUDED.release,
            carrier = EXCLUDED.carrier,
            manager = EXCLUDED.manager,
            comment = EXCLUDED.comment,
            status = EXCLUDED.status,
            weight_netto = EXCLUDED.weight_netto,
            updated_at = NOW();
    """)

    # Ð Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÐ¼ Ð½Ð° Ð¿Ð°Ñ‡ÐºÐ¸ Ð¿Ð¾ 500 Ð´Ð»Ñ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸
    batch_size = 500
    for i in range(0, len(rows), batch_size):
        batch = rows[i:i + batch_size]
        await session.execute(stmt, batch)
        await session.commit()

    logger.info(f"ðŸ’¾ [DB] ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Upsert Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½ Ð´Ð»Ñ {len(rows)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹.")

# --- ÐžÐ¡Ð¢ÐÐ›Ð¬ÐÐ«Ð• Ð¤Ð£ÐÐšÐ¦Ð˜Ð˜ (train_importer Ð¸ Ñ‚.Ð´.) Ð‘Ð•Ð— Ð˜Ð—ÐœÐ•ÐÐ•ÐÐ˜Ð™ ---
async def _collect_containers_from_excel(file_path: str) -> Dict[str, str]:
    xl = pd.ExcelFile(file_path)
    container_client_map = {}
    for sheet in xl.sheet_names:
        try:
            df = pd.read_excel(xl, sheet_name=sheet)
            df.columns = [str(c).strip() for c in df.columns]
            col_container = find_container_column(df)
            col_client = df.columns[CLIENT_COLUMN_INDEX] if len(df.columns) > CLIENT_COLUMN_INDEX else None
            if not col_container: continue
            for _, row in df.iterrows():
                cn = normalize_container(row.get(col_container))
                cl_val = clean_string_value(row.get(col_client)) if col_client else None
                if cn: container_client_map[cn] = cl_val if cl_val else ""
        except Exception as e:
            logger.error(f"Error reading sheet {sheet}: {e}")
    return container_client_map

async def import_train_from_excel(src_file_path: str) -> Tuple[int, int, str]:
    train_code = extract_train_code_from_filename(src_file_path)
    if not train_code: raise ValueError("No train code")
    container_map = await _collect_containers_from_excel(src_file_path)
    if not container_map: return 0, 0, train_code
    updated_count = 0
    async with SessionLocal() as session:
        async with session.begin():
            for cn, client_name in container_map.items():
                stmt = update(TerminalContainer).where(TerminalContainer.container_number == cn).values(train=train_code, client=client_name)
                res = await session.execute(stmt)
                updated_count += res.rowcount
    return updated_count, len(container_map), train_code