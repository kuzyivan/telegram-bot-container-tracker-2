# services/container_importer.py
from __future__ import annotations

import os
import re
from typing import List, Tuple, Iterable
import json

import pandas as pd
from sqlalchemy import text, insert
from sqlalchemy.dialects.postgresql import insert as pg_insert


from db import SessionLocal
from logger import get_logger
from models import TerminalContainer

logger = get_logger(__name__)


# -----------------------------------------------------------------------------
# –£—Ç–∏–ª–∏—Ç—ã (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
# -----------------------------------------------------------------------------

def extract_train_code_from_filename(filename: str) -> str | None:
    name = os.path.basename(filename)
    m = re.search(r"–ö\d{2}-\d{3}", name, flags=re.IGNORECASE)
    if not m:
        return None
    code = m.group(0)
    code = "–ö" + code[1:]
    return code


def normalize_container(value) -> str | None:
    if value is None:
        return None
    s = str(value).strip().upper()
    if not s or s == "NAN":
        return None
    s = re.sub(r"\s+", "", s)
    return s

def find_container_column(df: pd.DataFrame) -> str | None:
    for col in df.columns:
        c = str(col).strip().lower()
        if c in ["–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä", "container", "container #"]:
            return col
    return None


async def _collect_containers_from_excel(file_path: str) -> List[str]:
    if not os.path.exists(file_path):
        raise FileNotFoundError(file_path)

    xls = pd.ExcelFile(file_path)
    containers: List[str] = []

    for sheet in xls.sheet_names:
        try:
            df = pd.read_excel(file_path, sheet_name=sheet)
        except Exception:
            continue
        col = find_container_column(df)
        if not col:
            continue

        vals = [normalize_container(v) for v in df[col].dropna().tolist()]
        vals = [v for v in vals if v]
        containers.extend(vals)

    seen = set()
    uniq: List[str] = []
    for c in containers:
        if c not in seen:
            seen.add(c)
            uniq.append(c)
    return uniq


def _chunks(seq: Iterable[str], size: int) -> Iterable[List[str]]:
    buf: List[str] = []
    for x in seq:
        buf.append(x)
        if len(buf) >= size:
            yield buf
            buf = []
    if buf:
        yield buf

# -----------------------------------------------------------------------------
# –§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø –§–£–ù–ö–¶–ò–ò –ò–ú–ü–û–†–¢–ê
# -----------------------------------------------------------------------------

async def import_loaded_and_dispatch_from_excel(file_path: str) -> Tuple[int, int]:
    if not os.path.exists(file_path):
        raise FileNotFoundError(file_path)

    COLUMN_MAP = {
        '–¢–µ—Ä–º–∏–Ω–∞–ª': 'terminal', '–ó–æ–Ω–∞': 'zone', '–ö–ª–∏–µ–Ω—Ç': 'client',
        '–°—Ç–æ–∫': 'stock', '–¢–∞–º–æ–∂–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º': 'customs_mode',
        '–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ': 'destination_station', '–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ': 'note',
    }

    xls = pd.ExcelFile(file_path)
    target_sheets = [
        s for s in xls.sheet_names
        if str(s).strip().lower().startswith(("dispatch", "loaded"))
    ]

    total_changed = 0
    processed_sheets = 0

    async with SessionLocal() as session:
        for sheet in target_sheets:
            try:
                df = pd.read_excel(file_path, sheet_name=sheet)
                df.columns = [str(c).strip() for c in df.columns]
                
                container_col_name = find_container_column(df)
                if not container_col_name:
                    logger.warning(f"[Executive summary] –ù–∞ –ª–∏—Å—Ç–µ '{sheet}' –Ω–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü —Å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞–º–∏.")
                    continue

                for _, row in df.iterrows():
                    container_num = normalize_container(row.get(container_col_name))
                    if not container_num:
                        continue
                    
                    # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏/–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
                    data_to_upsert = {'container_number': container_num}
                    for xl_col, db_col in COLUMN_MAP.items():
                        if xl_col in row:
                            value = row[xl_col]
                            data_to_upsert[db_col] = '' if pd.isna(value) else str(value)
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤ SQLAlchemy - —ç—Ç–æ –Ω–∞–¥—ë–∂–Ω–µ–µ
                    stmt = pg_insert(TerminalContainer).values(data_to_upsert)
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ –ø–æ–ª—è –æ–±–Ω–æ–≤–ª—è—Ç—å –ø—Ä–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–µ
                    update_data = {k: v for k, v in data_to_upsert.items() if k != 'container_number'}
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º ON CONFLICT ... DO UPDATE
                    # –≠—Ç–æ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —á—Ç–æ –æ–±–Ω–æ–≤–ª—è—Ç—å
                    if update_data:
                        stmt = stmt.on_conflict_do_update(
                            index_elements=['container_number'],
                            set_=update_data
                        )
                    else:
                        stmt = stmt.on_conflict_do_nothing(
                            index_elements=['container_number']
                        )

                    await session.execute(stmt)
                    total_changed +=1 # –°—á–∏—Ç–∞–µ–º –∫–∞–∂–¥—É—é –ø–æ–ø—ã—Ç–∫—É –≤—Å—Ç–∞–≤–∫–∏/–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è

                await session.commit()
                processed_sheets += 1

            except Exception as e:
                logger.error(f"[Executive summary] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–∏—Å—Ç–∞ '{sheet}': {e}", exc_info=True)
                await session.rollback()

    logger.info(f"üì• –ò–º–ø–æ—Ä—Ç Executive summary: –ª–∏—Å—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ={processed_sheets}, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π={total_changed}")
    return total_changed, processed_sheets


# -----------------------------------------------------------------------------
# –ò–º–ø–æ—Ä—Ç ¬´–ø–æ–µ–∑–¥–Ω—ã—Ö¬ª —Ñ–∞–π–ª–æ–≤ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
# -----------------------------------------------------------------------------

async def import_train_excel(src_file_path: str) -> Tuple[int, int, str]:
    if not os.path.exists(src_file_path):
        raise FileNotFoundError(src_file_path)

    train_code = extract_train_code_from_filename(src_file_path)
    if not train_code:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–¥ –ø–æ–µ–∑–¥–∞ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞. –û–∂–∏–¥–∞–µ—Ç—Å—è —à–∞–±–ª–æ–Ω '–ö–î–î-–ù–ù–ù'.")

    containers = await _collect_containers_from_excel(src_file_path)
    total = len(containers)
    if total == 0:
        logger.info(f"[Train] –í —Ñ–∞–π–ª–µ –Ω–µ—Ç –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤: {os.path.basename(src_file_path)}")
        return 0, 0, train_code

    updated_sum = 0
    async with SessionLocal() as session:
        for chunk in _chunks(containers, 500):
            res = await session.execute(
                text("""
                    UPDATE terminal_containers
                       SET train = :train
                     WHERE container_number = ANY(:cn_list)
                """),
                {"train": train_code, "cn_list": chunk},
            )
            updated_sum += res.rowcount or 0

        await session.commit()

    logger.info(f"üöÜ –ü—Ä–æ—Å—Ç–∞–≤–ª–µ–Ω –ø–æ–µ–∑–¥ {train_code}: –æ–±–Ω–æ–≤–ª–µ–Ω–æ {updated_sum} –∏–∑ {total} –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ "
                f"({os.path.basename(src_file_path)})")
    return updated_sum, total, train_code